{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57facccd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f90a383bbe8ef014db0f587cd6e18436",
     "grade": false,
     "grade_id": "cell-0ce69fe260e748d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercice 13) Deep Deterministic Policy Gradients and Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe402026",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e358d9cd970ad535b7c16a0bd726250",
     "grade": false,
     "grade_id": "cell-5be3dd74ab09bc50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise we will investigate two state-of-the-art algorithms: deep deterministic policy gradient (DDPG) and proximal policy optimization (PPO).\n",
    "\n",
    "We will examine their performance on [Goddard's rocket problem](https://github.com/osannolik/gym-goddard).\n",
    "This environment comes prepackaged in this notebook's folder, so it can be just imported.\n",
    "\n",
    "```\n",
    "First formulated by R. H. Goddard around 1910, this is a classical problem within dynamic optimization and optimal control. The task is simply to find the optimal thrust profile for a vertically ascending rocket in order for it to reach the maximum possible altitude, given that its mass decreases as the fuel is spent and that it is subject to varying drag and gravity.\n",
    "```\n",
    "\n",
    "The gym's observation space is the rocket's vertical position, velocity and mass.\n",
    "\n",
    "The rocket engine is assumed to be throttled such that the thrust can be continuously controlled between 0 to some maximum limit, which translates to an action space $\\mathcal A \\in [0, 1]$.\n",
    "\n",
    "![](rocket_spacex.jpg)\n",
    "(Photo by <a href=\"https://unsplash.com/@spacex?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">SpaceX</a> on <a href=\"https://unsplash.com/s/photos/rocket?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>)\n",
    "  \n",
    "\n",
    "In order for the full notebook to run through, you will also need the stable-baslines3 package.\n",
    "\n",
    "```\n",
    "pip install stable-baselines3[extra]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccabd97",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b367e25d3afb72b88667dda4b6996395",
     "grade": false,
     "grade_id": "cell-f7ba0aeae957d480",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from rocket_env import GoddardEnv\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from stable_baselines3 import PPO, DDPG, A2C\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from copy import deepcopy\n",
    "\n",
    "OPTIMAL_CONTROL = 0.0122079818367078\n",
    "RANDOM_AVG = 0.00995\n",
    "RANDOM_BEST = 0.01148"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3a5c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd741a120aa2f2478666d183ca1542a9",
     "grade": false,
     "grade_id": "cell-d77d2440cb4fa969",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb652100",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9e8517bd08e724e13c9764b1b4e45c7",
     "grade": false,
     "grade_id": "cell-027671c6914671f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Deep Deterministic Policy Gradient (DDPG) was first introduced [2015](https://arxiv.org/abs/1509.02971).\n",
    "It uses four neural networks.\n",
    "The first two we already know from the last exercise introduced Actor-Critic method: a critic to estimate the q-value function and an actor which deterministicly gives us an action for a specific state.\n",
    "Additionally DDPG provides two target networks which are (in the beginning) copies of tthe actor and critic. These target networks are updated time-delayed in a low-pass filter manner.\n",
    "This enhances stability during the learing process.\n",
    "For more information see for example [here](https://spinningup.openai.com/en/latest/algorithms/ddpg.html).\n",
    "\n",
    "During the learning process the critic is updated based on minimizing (!) the follwoing loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "L(w, {\\mathcal D}) = \\underset{(x,u,r,x',d) \\sim {\\mathcal D}}{{\\mathrm E}}\\left[\n",
    "    \\Bigg( \\hat{q}(x,u, w) - \\left(r + \\gamma (1 - d)  \\hat{q}_{}(x',\\pi(x', \\theta_\\text{target}), w_\\text{target}) \\right) \\Bigg)^2\n",
    "    \\right].\n",
    "\\end{equation}   \n",
    "\n",
    ", here the target is calculated using the target networks. $w$ and $w_{target}$ are the parameters of the critic and critic-target networks $\\hat{q}$, respectively and $\\theta_{target}$ define the parameters of the actor-target network.\n",
    "\n",
    "The policy/actor network is updated based on the idea to maximize (!) the expected return\n",
    "\n",
    "\\begin{align}\n",
    "    \\max_{\\theta} \\underset{s \\sim {\\mathcal D}}{{\\mathrm E}}\\left[\\hat{q}(x, \\pi_{}(x, \\theta), w) \\right]\n",
    "\\end{align}  \n",
    "\n",
    ", here $\\theta$ are the parameters of the policy network $\\pi$.\n",
    "\n",
    "The updates are performed off-policy by sampling from an experince replay buffer ${\\mathcal D}$.\n",
    "Since we are dealing with an deterministic actor, exploration during training is achieved by adding noise to the sampled actions.\n",
    "\n",
    "### Task: Implement the DDPG with Pytorch\n",
    "Write an DDPG algorithm using the algXXX in lecture slides 13!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d07d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d5fef16377bb23d62de4e916a66f74d",
     "grade": false,
     "grade_id": "cell-947a0cd7668e21df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Execute the following cell to make use of the defined multi layer perceptron and the plot function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e71f6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35f55ca3a0b374f1280e299441c255b0",
     "grade": false,
     "grade_id": "cell-8303cf3b2bb624b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    \"\"\"\n",
    "    Defines a multi layer perceptron using pytorch layers and activation funtions\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def plot_reward_trends(logs):\n",
    "    repeats = len(logs['envs'])\n",
    "    ncols = 2\n",
    "    nrows = int(np.ceil(repeats / ncols))\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*8, nrows*4), sharex=True, sharey=True)\n",
    "    for ax, _env, _model in zip(axes.flatten(), logs['envs'], logs['models']):\n",
    "        ax.plot(_env.get_episode_rewards(), label='rewards', color='green')\n",
    "\n",
    "        ax.set_ylabel('Reward')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylim(0, 0.015)\n",
    "        ax.axhline(OPTIMAL_CONTROL, ls='--', color='red', label='optimal')\n",
    "        ax.axhline(RANDOM_BEST, ls='--', color='orange', label='best rnd actions')\n",
    "        ax.axhline(RANDOM_AVG, ls='--', color='yellow', label='average rnd actions')\n",
    "        ax.legend(loc='lower left')\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(_env.get_episode_lengths(), label='ep lengths')\n",
    "        ax2.legend()\n",
    "        ax2.set_ylabel('Steps')\n",
    "    fig.tight_layout()\n",
    "\n",
    "def test_agent(logs):\n",
    "    \"\"\"Deterministic test of the agent in the given env\"\"\"    \n",
    "    num_test_episodes = 400\n",
    "    for i, (env, agent) in enumerate(zip(logs['envs'], logs['models'])):\n",
    "        episode_reward = 0\n",
    "        episode_len = 0\n",
    "        state = env.reset()\n",
    "        for j in tqdm(range(num_test_episodes)):\n",
    "            episode_len += 1\n",
    "            action = agent.decide(state, deterministic=True)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            env.render()\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                break\n",
    "\n",
    "        print(f'Reward during test ({i}): {episode_reward}')\n",
    "        env.close()\n",
    "    print('Optimal control: 0.0122')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad278f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab0612d0ba0fa65b47af356f936f4d1e",
     "grade": false,
     "grade_id": "cell-b4bd4f5ae4c71df8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Execute the following cells to make use of the predefine actor & critic and the replay buffer!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90348b4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9720f9408684f00f449b0bbf12ba4ff1",
     "grade": false,
     "grade_id": "cell-93290e96cc879232",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation=nn.ReLU, act_limit=1):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Sigmoid)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.act_limit * self.pi(state)\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.act_limit * self.pi(state).numpy()\n",
    "    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # torch.cat concatenates action to state\n",
    "\n",
    "        q = self.q(torch.cat([state, action], dim=-1))\n",
    "        return torch.squeeze(q, -1)  # To ensure q has right shape.\n",
    "    \n",
    "class ReplayBuffer:\n",
    "\n",
    "        def __init__(self, obs_dim, action_dim, buffer_size):\n",
    "            \n",
    "            self.state_buf = np.zeros((buffer_size, obs_dim), dtype=np.float32)\n",
    "            self.next_state_buf = np.zeros((buffer_size, obs_dim), dtype=np.float32)\n",
    "            self.action_buf = np.zeros((buffer_size, action_dim), dtype=np.float32)\n",
    "            self.reward_buf = np.zeros(buffer_size, dtype=np.float32)\n",
    "            self.done_buf = np.zeros(buffer_size, dtype=np.float32)\n",
    "            self.ptr, self.size, self.max_size = 0, 0, buffer_size\n",
    "\n",
    "        def push(self, state, action, reward, next_state, done):\n",
    "            \n",
    "            self.state_buf[self.ptr] = state\n",
    "            self.next_state_buf[self.ptr] = next_state\n",
    "            self.action_buf[self.ptr] = action\n",
    "            self.reward_buf[self.ptr] = reward\n",
    "            self.done_buf[self.ptr] = done\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "            self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "        def fetch(self, batch_size=32):\n",
    "            \n",
    "            idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "            \n",
    "            batch = dict(state=self.state_buf[idxs],\n",
    "                         next_state=self.next_state_buf[idxs],\n",
    "                         action=self.action_buf[idxs],\n",
    "                         reward=self.reward_buf[idxs],\n",
    "                         done=self.done_buf[idxs])\n",
    "            \n",
    "            return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfe16e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb863c769b97307b480cb8709aed025f",
     "grade": false,
     "grade_id": "cell-2639a410b1080568",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here, fill in the following code template to write a DDPG agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edca219",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75a23ae85e9194bf3e18d11594dd7c05",
     "grade": false,
     "grade_id": "cell-fbb8e1d539cd0caf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DDPG_agent:\n",
    "    \"\"\"Reference:\n",
    "    https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, env, actor_hidden_size, actor_number_layers, critic_hidden_size, critic_number_layers,\n",
    "                 buffer_size, actor_lr, critic_lr, gamma, batch_size, learning_starts, tau):#, action_noise):\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_starts = learning_starts\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.shape[0]\n",
    "        \n",
    "\n",
    "        # define A&C and the replaybuffer        \n",
    "        self.actor = None\n",
    "        self.critic = None\n",
    "        self.replay_buffer = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # defines target networks\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        \n",
    "        # Uses Adam optimizer (see to ex12 for more explanation)\n",
    "        self.actor_optimizer = Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        # define action noise space for clipping\n",
    "        self.action_space_high = env.action_space.high\n",
    "        self.action_space_low = env.action_space.low\n",
    "\n",
    "    def q_loss(self, data):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def pi_loss(self, data):\n",
    "        \"\"\"\n",
    "        Calulate the loss for a gradient ascent(!) step\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    \n",
    "    def deliberate(self, number_updates_per_step):\n",
    "        \"\"\"\n",
    "        Fetches number_updates_per_step-times from replay_buffer, computes targets and calculates losses to \n",
    "        update the q-function using gradient descent and the policy function using gradient ascent\n",
    "        In the end updates the target networks\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(number_updates_per_step):\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    def decide(self, state, deterministic=False, noise_scale=0.1):\n",
    "        \"\"\"\n",
    "        Returns action as nd-array depending on the state, adds scaled noise \n",
    "        and clipps the action depending on the actionspace\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaacefb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58b41c760d444b3dfc0d242437e4fff3",
     "grade": false,
     "grade_id": "cell-f1983e65eedd72ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the following cell to train your agent. Note that, in contrast to classic RL applications, we update the agent only after full episodes due to the sparse reward signal of the rocket environment instead of after each environment step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde17eef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b625f86266dcaabead3ba7bdb338b170",
     "grade": false,
     "grade_id": "cell-c7977ed401a0f23b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_logs_ddpg = {'envs': [], 'models': []}\n",
    "repeats = 4\n",
    "\n",
    "total_timesteps = 20000\n",
    "\n",
    "for rep in range(repeats):\n",
    "\n",
    "\n",
    "    env = Monitor(GoddardEnv())\n",
    "\n",
    "    myDDPG_agent = DDPG_agent(env=env, actor_hidden_size=8, actor_number_layers=1,\n",
    "                           critic_hidden_size=8, critic_number_layers=1, buffer_size=int(1e6),\n",
    "                           actor_lr=1e-4, critic_lr=1e-4, gamma=0.999, batch_size=256, learning_starts=100, tau=0.005)\n",
    "\n",
    "    number_updates_per_step = 0\n",
    "    episode_reward = 0\n",
    "    episode_len = 0\n",
    "    rewards = []\n",
    "    episode_len_vec = []\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    for j in tqdm(range(total_timesteps)):\n",
    "        episode_len += 1\n",
    "        number_updates_per_step += 1\n",
    "        action = myDDPG_agent.decide(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        myDDPG_agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        #env.render()\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            myDDPG_agent.deliberate(number_updates_per_step)\n",
    "            state = env.reset()\n",
    "            rewards.append(episode_reward)\n",
    "            episode_len_vec.append(episode_len)\n",
    "            episode_reward = 0\n",
    "            episode_len = 0\n",
    "            number_updates_per_step = 0\n",
    "\n",
    "    train_logs_ddpg['envs'] += [env]\n",
    "    train_logs_ddpg['models'] += [myDDPG_agent]\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f142c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "191a3bcde803503f3395412ea118c2f9",
     "grade": false,
     "grade_id": "cell-f63074dfa3920133",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_reward_trends(train_logs_ddpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec9414",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a98f456bc82cabf59e8411637b246652",
     "grade": false,
     "grade_id": "cell-04b7fa3a7ef613f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following cell to test your agent on the env using deterministic actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf718c04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab9596f4fcfc776ec8d186b782a377db",
     "grade": false,
     "grade_id": "cell-b85d8e3dd9ad383b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_agent(train_logs_ddpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa062069",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d7b903774b981b858831e969746c1c7",
     "grade": false,
     "grade_id": "cell-57ec48fad41194c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Demo: StableBaselines3 usage\n",
    "\n",
    "Alternatively, in the real world, you would use readily available Python packages such as [stable-baselines3](https://github.com/DLR-RM/stable-baselines3) for employment of state-of-the-art algorithms.\n",
    "In what follows below, the DDPG algorithm as utilized by stable-baselines3 is showcased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1eaafa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "837f45584f89640708470849b387cfc1",
     "grade": false,
     "grade_id": "cell-4bc64d70ae9709c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "total_timesteps = int(20e3)\n",
    "\n",
    "\n",
    "class Pbar(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pbar = tqdm(desc='Training', total=total_timesteps)\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"This event is triggered before updating the policy.\"\"\"\n",
    "        # self.pbar.update(self.)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.pbar.update()\n",
    "        return True\n",
    "    \n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[8], vf=[8])])\n",
    "\n",
    "repeats = 5\n",
    "logs = {'envs': [], 'models': []}\n",
    "for _ in range(repeats):\n",
    "    \n",
    "    env = Monitor(GoddardEnv())\n",
    "    \n",
    "    #n_actions = env.action_space.shape[-1]\n",
    "    #noise_var = 4\n",
    "    #noise_theta = 25  # stiffness of OU\n",
    "    #action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), theta=noise_theta * np.ones(n_actions),\n",
    "    #                                        sigma=noise_var * np.ones(n_actions))\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "    \n",
    "    policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=dict(pi=[8] * 1, qf=[8] * 1))\n",
    "    model = DDPG(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, batch_size=256, device='cpu',\n",
    "             learning_rate=1e-4, gamma=0.999)\n",
    "    model.learn(total_timesteps=total_timesteps, callback=Pbar())\n",
    "    logs['envs'] += [env]\n",
    "    logs['models'] += [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2243e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e54f4ad98a1a9a08b66c45796640c6b7",
     "grade": false,
     "grade_id": "cell-354440d67e2e0b95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_reward_trends(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a1387f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6683a41cd6cd9b421105c1c4af506f94",
     "grade": false,
     "grade_id": "cell-f7ad3146465beb22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following cell to execute the last trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47da32",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e947e8b77505f0eabf09f9d64d32bf4",
     "grade": false,
     "grade_id": "cell-c25a8b2418b0a9a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_episodes = 1\n",
    "max_steps_per_episode = 500\n",
    "tst_logs = {'rewards': []}\n",
    "for ep in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = np.zeros(1, dtype=np.bool)\n",
    "    cum_rew = 0\n",
    "    k = 0\n",
    "    while not np.all(done) and k < max_steps_per_episode:\n",
    "        action, _ = model.predict(state, deterministic=True) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        cum_rew += reward\n",
    "        k += 1\n",
    "\n",
    "    tst_logs['rewards'].append(cum_rew)\n",
    "rewards = np.array(tst_logs['rewards']).ravel()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50f7af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff21249a4b8012e5ac6d0bda8cd00879",
     "grade": false,
     "grade_id": "cell-038ec2a3065ebfd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) PPO\n",
    "\n",
    "The [original paper from 2017](https://arxiv.org/abs/1707.06347) for the PPO came up with an idea to combine A2C (having multiple workers) and TRPO (using a trust region to improve the actor).\n",
    "The PPO algorithm achieves this by hard clipping gradients in order to ensure that new policies won't be too far away from old ones.\n",
    "\n",
    "In contrast to DDPG, PPO is an on-policy algorithm. In order to still apply mini-batch training, there is a so-called roll-out-buffer that is filled up with the current policy, on whose base a gradient ascent update would be done.\n",
    "\n",
    "There are [two variants on the PPO algorithm](https://spinningup.openai.com/en/latest/algorithms/ppo.html), from which we will implement the clip variant.\n",
    "\n",
    "The actor (policy) update is computed according to\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_{k+1} &= \\arg \\max_{\\theta} \\underset{\\langle x,u \\rangle \\sim \\pi_{\\theta_k}}{{\\mathbb E}}\\left[\\mathcal L(X,U,\\theta_k, \\theta)\\right], \\\\\n",
    "\\mathcal L(x,u,\\theta_k,\\theta) &= \\min\\left(\n",
    "\\frac{\\pi_{\\theta}(u|x)}{\\pi_{\\theta_k}(u|x)}  A^{\\pi_{\\theta_k}}(x,u), \\;\\;\n",
    "\\text{clip}\\left(\\frac{\\pi_{\\theta}(u|x)}{\\pi_{\\theta_k}(u|x)}, 1 - \\epsilon, 1+\\epsilon \\right) A^{\\pi_{\\theta_k}}(x, u)\n",
    "\\right),\n",
    "\\end{align}\n",
    "where the expectation operator denotes the empirical average across the roll-out-buffer, $\\theta$ the possible next weights of the actor, $\\epsilon$ denotes the threshold within the next update is allowed, and $\\mathcal L$ the long-term return.\n",
    "\n",
    "On the opposite side, the critic's weights $\\omega_k$ are updated through the plain stochastic mini-batch gradient descent on the mean squared error $\\mathcal C$ between immediately seen rewards $r_{k+1}$ and corresponding estimated value $v_{\\phi_k}(x_k)$ across the roll-out-buffer:\n",
    "\\begin{align}\n",
    "\\phi_{k+1} &\\leftarrow \\omega_k - \\eta \\nabla\\mathcal C(X,R_{k+1},\\omega_k), \\\\\n",
    "\\mathcal C(x,r_{k+1}, \\omega_k) &= \\left(v_{\\omega_k} (x_k)- r_{k+1}\\right)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402a64a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b76229a9e2e9cdf857899d2d4a2eefa",
     "grade": false,
     "grade_id": "cell-37e0d6f3e79fb7f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task: Implement the PPO clip variant with Pytorch\n",
    "Fill in the below code to run the PPO-clip variant on the rocket environment.\n",
    "\n",
    "For simplicity, do not contemplate a vectorized environment for synchronous training (like in A2C or A3C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1722f5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c6b15a3cfe8a0b01f1f749190a3122d",
     "grade": false,
     "grade_id": "cell-bfaedbfa4a92491d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class StochActor(nn.Module):\n",
    "    \"\"\"This stochastic actor learns a Gaussian distribution. While the mean value \n",
    "    is learnt by a full-fledged MLP, the standard deviation is denoted by a\n",
    "    single trainable weight. With Pytorch's distribution package, probabilities \n",
    "    given a certain distribution and an action can be calculated.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # The standard deviation is just one trainable weight\n",
    "        self.log_std = torch.nn.Parameter(\n",
    "                        torch.as_tensor(\n",
    "                            -.5 * np.ones(action_dim, dtype=np.float32)))\n",
    "        # the mean value is estimated by a full MLP\n",
    "        self.mu_net = mlp(list(state_dim) + [8] + list(action_dim), nn.Tanh, nn.Sigmoid)\n",
    "        \n",
    "    def _distribution(self, state):\n",
    "        return torch.distributions.normal.Normal(\n",
    "                self.mu_net(state), torch.exp(self.log_std))\n",
    "    \n",
    "    def forward(self, state, action=None):\n",
    "        pi = self._distribution(state)\n",
    "        # if action is None, logp_a will be, too\n",
    "        if action is None:\n",
    "            logp_a = None\n",
    "        else:\n",
    "            logp_a = pi.log_prob(action).sum(axis=-1)\n",
    "        return pi, logp_a\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"Reference:\n",
    "    https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ppo/ppo.py\n",
    "    \"\"\"\n",
    "    \n",
    "    class RolloutBuffer:\n",
    "        def __init__(self, size, action_dim, state_dim):\n",
    "            self.action_buf = np.zeros((size, action_dim[0]), dtype=np.float32)\n",
    "            self.state_buf = np.zeros((size, state_dim[0]), dtype=np.float32)\n",
    "            self.rew_buf = np.zeros(size+1, dtype=np.float32)\n",
    "            self.val_buf = np.zeros(size+1, dtype=np.float32)\n",
    "            self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "            self.i = 0\n",
    "            self.max_size = size\n",
    "            \n",
    "        def push(self, state, action, reward, value, logp):\n",
    "            \"\"\"Append a sample to the buffer\"\"\"\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        def fetch(self, last_val=None, gamma=0.999):\n",
    "            \"\"\"Get all data from the rollout buffer.\n",
    "            Returns a dictionary for state, action, rewards-to-go, advantages, and logp values.\n",
    "            Entries might be of different length across multiple calls to this function as episodes\n",
    "            potentially have different lengths. Rewards-to-go and advantages need to be computed from \n",
    "            rewards, values, and gamma, as outlined in https://arxiv.org/abs/1506.02438 (or as done\n",
    "            in the spinning up implementation). Normalize the advantage batch by standard scaling.\"\"\"\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def __init__(self, buffer_size, env):\n",
    "        self.buf = self.RolloutBuffer(buffer_size, action_dim=env.action_space.shape,\n",
    "                                 state_dim=env.observation_space.shape)\n",
    "        # use the below attributes for the actor-critic agent\n",
    "        self.actor = None\n",
    "        self.critic = None\n",
    "        self.n_epochs = 8\n",
    "        self.clip_ratio = 0.1\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def v_loss(self, data):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def pi_loss(self, data):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def deliberate(self, last_value=None):\n",
    "        \"\"\"Fetch the rollout buffer for all samples.\n",
    "        Train the actor and critic for n_epochs by computing the pi and v\n",
    "        loss iteratively.\n",
    "        The rewards and values buffer needs a zero appended for terminal states,\n",
    "        or the critic's estimate appended in case of intermediate states \n",
    "        that result from timeout. This is denoted by last_value.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def decide(self, state, deterministic=False):\n",
    "        \"\"\"Receive a state as torch tensor and return a tuple of numpy ndarrays\n",
    "        in the form (action, value, log-probability-of-action-under-pi)\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf33ec1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16693633eb25e4f7718340808b1da5b1",
     "grade": false,
     "grade_id": "cell-4cfa3ab6af86faef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# main training loop\n",
    "\n",
    "n_episodes = 250\n",
    "max_steps_per_episode = 500\n",
    "train_logs_ppo_custom = {'envs': [], 'models': []}\n",
    "repeats = 4\n",
    "for rep in range(repeats):\n",
    "    env = Monitor(GoddardEnv())\n",
    "    agent = PPOAgent(max_steps_per_episode, env)\n",
    "\n",
    "    for ep in tqdm(range(n_episodes), desc='Training'):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        k = 0\n",
    "        while not done and k < max_steps_per_episode:\n",
    "            action, value, logp_a = agent.decide(state) \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # track experience\n",
    "            agent.buf.push(state, action, reward, value, logp_a)\n",
    "\n",
    "            state = next_state\n",
    "            k += 1\n",
    "\n",
    "        # if episode finished with done signal, the long term value should be 0\n",
    "        #  for this terminal state\n",
    "        if done:\n",
    "            last_value = None\n",
    "        else:\n",
    "            _, last_value, _ = self.decide(state)\n",
    "        agent.deliberate(last_value)\n",
    "\n",
    "    train_logs_ppo_custom['envs'] += [env]\n",
    "    train_logs_ppo_custom['models'] += [agent]\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ba6e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aafaa7441e23545b17ddb5e9093626b9",
     "grade": false,
     "grade_id": "cell-ec0e800aac84b980",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_reward_trends(train_logs_ppo_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70642580",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "014a0598a117294cb426c82ad3920a0d",
     "grade": false,
     "grade_id": "cell-f3ba7a0d376cc7ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_agent(train_logs_ppo_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc04c0d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb688f9eedf8dd703a8fb33a3b8965fd",
     "grade": false,
     "grade_id": "cell-cd9a6dc2f628c73d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Demo: StableBaselines3 usage\n",
    "\n",
    "Alternatively, in the real world, you would use readily available Python packages such as [stable-baselines3](https://github.com/DLR-RM/stable-baselines3) for employment of state-of-the-art algorithms.\n",
    "In what follows below, the PPO algorithm as utilized by stable-baselines3 is showcased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d06d09",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f42a16e368f4532cf2db06821ec1e09b",
     "grade": false,
     "grade_id": "cell-b91e000d33246cbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Pbar(BaseCallback):\n",
    "    \"\"\"This is a callback that helps monitoring the training progress\"\"\"\n",
    "    def __init__(self, total, n_rollout_steps):\n",
    "        super().__init__()\n",
    "        self.n_steps = n_rollout_steps\n",
    "        self.pbar = tqdm(desc='Training', total=total)\n",
    "        \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"This event is triggered before updating the policy.\"\"\"\n",
    "        self.pbar.update(self.n_steps)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    \n",
    "policy_kwargs = dict(activation_fn=torch.nn.Sigmoid, net_arch=[dict(pi=[8], vf=[8])])\n",
    "\n",
    "n_steps = 256  # an episode takes around ~250 steps\n",
    "total_timesteps = n_steps*250  # steps_per_episode * episodes\n",
    "\n",
    "repeats = 5  # repeat experiment x times to assess scatter through random init\n",
    "train_logs_ppo_sb3 = {'envs': [], 'models': []}\n",
    "for _ in range(repeats):\n",
    "    \n",
    "    env = Monitor(GoddardEnv())\n",
    "    model = PPO('MlpPolicy', env, n_steps=n_steps, n_epochs=10,  verbose=0, device='cpu', learning_rate=1e-4, batch_size=n_steps, policy_kwargs=policy_kwargs,\n",
    "               clip_range=0.1, clip_range_vf=0.1, gamma=0.999)\n",
    "    model = model.learn(total_timesteps=total_timesteps, callback=Pbar(total_timesteps, n_steps))\n",
    "    train_logs_ppo_sb3['envs'] += [env]\n",
    "    train_logs_ppo_sb3['models'] += [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6964c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87d622f1dfc171c14b7d6ce3cf2ea659",
     "grade": false,
     "grade_id": "cell-531e062c849ee81b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_reward_trends(train_logs_ppo_sb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694b064",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a8a4d779b61587dab6f6027f6408f52",
     "grade": false,
     "grade_id": "cell-78eedb7acc89df22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Obviously, the chosen architecture and hyper parameters do well often, but can also fail abruptly. \n",
    "What might be the reason for these failures?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
