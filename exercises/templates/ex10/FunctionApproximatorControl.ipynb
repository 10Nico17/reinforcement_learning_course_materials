{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cc9e5594135424eaa186c0e58284423",
     "grade": false,
     "grade_id": "cell-71ced7bc2b82b03e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 10) Function Approximators in Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4bfd741a5365dca466ab20e77a05062b",
     "grade": false,
     "grade_id": "cell-02f5bf72a108e92e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After looking at function-approximator prediction we are now ready to investigate function-approximator control. This topic is still being researched and there are many interesting applications, but most of them are not yet used economically because experts in reinforcement learning are scarce. Control tasks are generally more complex than prediction tasks and we often need to apply domain-specific expert knowledge and very performant algorithms to succeed with function approximators.\n",
    "\n",
    "As in the last exercise we will have a look at the MountainCar from OpenAI's `gym`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97e91349cf12c2c59dac8c4746bae41a",
     "grade": false,
     "grade_id": "cell-0f849f94f22b5741",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please make sure to have `sklearn` installed:\n",
    "\n",
    "`pip install sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7c8957fa7039f6d2d05e6dbac46075c",
     "grade": false,
     "grade_id": "cell-9bd695d3e4934221",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-talk')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import deque\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "815cd1f9f7cef3c969a52392e75beab9",
     "grade": false,
     "grade_id": "cell-d37cf10e4548222c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Artificial Neural Networks for Semi-Gradient SARSA Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0212267adeb40b3a35a34094e2639f33",
     "grade": false,
     "grade_id": "cell-d7889fafaa72e858",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the start, we will try to control the MountainCar in the same fashion that enabled us to predict the value function in the last exercise. You can refer to the last exercise to see how `Tensorflow` can be used here. Write a learning algorithm that utilizes the given ANN setup to control the MountainCar. Episodes will terminate automatically after $200$ timesteps if the finish line has not been reached. Since every timestep is rewarded with $r=-1$, the goal should be to reach the finish line as fast as possible. \n",
    "\n",
    "State space: $x = \\begin{bmatrix}\\text{position} \\\\ \\text{velocity}\n",
    "\\end{bmatrix} \\in \n",
    "\\begin{bmatrix}[-1.2, 0.6] \\\\ [-0.07, 0.07]\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Feature vector: $\\tilde{x} \\in \n",
    "\\begin{bmatrix}[-1, 1] \\\\ [-1, 1]\n",
    "\\end{bmatrix}$ (you may want to change or extend these features)\n",
    "\n",
    "Input space: $u \\in \\{ 0 \\text{ (left)}, 1 \\text{ (idle)}, 2 \\text{ (right)}\\}$\n",
    "\n",
    "We will give the same features as in the last exercise: simply MinMax-normalized states. Feel free to enhance this feature vector in order to improve the trainability of this algorithm.\n",
    "\n",
    "As explained, control problems with the use of function approximators tend to be quite hard. If you have the feeling of wasting your time, please proceed to the next task before giving up this exercise.\n",
    "\n",
    "![](https://media.giphy.com/media/xUPJPgq5NcxbKHrcXK/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae09782a77f6b8e8aba3718457c391c0",
     "grade": false,
     "grade_id": "cell-76d1ee608451b89f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following two cells will implement the `featurize` and the `plot_surface` function. The latter can be used to monitor the quality of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df855f558922bd057a71fd5f4712adfd",
     "grade": false,
     "grade_id": "cell-4335ae0661f1e127",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def featurize(state):\n",
    "    pos = state[0]\n",
    "    pos_min = -1.2\n",
    "    pos_max = +0.6\n",
    "    \n",
    "    vel = state[1]\n",
    "    vel_min = -0.07\n",
    "    vel_max = +0.07\n",
    "    \n",
    "    norm_pos = (pos - pos_min) / (pos_max - pos_min) * 2 - 1\n",
    "    norm_vel = (vel - vel_min) / (vel_max - vel_min) * 2 - 1\n",
    "    \n",
    "    return np.array([[norm_pos, norm_vel]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e081e863a741d284097b78a29750ca8",
     "grade": false,
     "grade_id": "cell-794d829ffd7e9403",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_surface(model, input_dim, visited_states):\n",
    "    resolution = 100\n",
    "    pos_vec = np.linspace(-1.2, 0.6, resolution)\n",
    "    vel_vec = np.linspace(-0.07, 0.07, resolution)\n",
    "\n",
    "    pos_mat, vel_mat = np.meshgrid(pos_vec, vel_vec)\n",
    "    state_tensor = np.zeros([resolution, resolution, input_dim])\n",
    "\n",
    "    for pos_idx, pos in enumerate(tqdm(pos_vec)):\n",
    "        for vel_idx, vel in enumerate(vel_vec):\n",
    "            state_tensor[vel_idx, pos_idx] = featurize(np.array([pos, vel]))\n",
    "\n",
    "    q_mat = model(np.reshape(state_tensor, (-1, 2)))\n",
    "    q_maxes = np.reshape(np.max(q_mat, axis=1), (resolution, resolution))\n",
    "\n",
    "    normalized_visited = np.zeros([len(visited_states[:, 0]), input_dim])\n",
    "    for idx, (pos, vel) in enumerate(\n",
    "            tqdm(zip(visited_states[:, 0], visited_states[:, 1]), total=len(visited_states[:, 0]))):\n",
    "        normalized_visited[idx] = featurize(np.array([pos, vel]))\n",
    "\n",
    "    visited_values = model(np.array(normalized_visited))\n",
    "    visited_values = np.max(visited_values, axis=1)\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.subplots()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(pos_mat, vel_mat, -q_maxes, cmap=\"viridis\")\n",
    "    ax.scatter(visited_states[:, 0], visited_states[:, 1], -visited_values - 0.05, color=\"red\")\n",
    "    ax.set_xlabel('\\n\\nposition')\n",
    "    ax.set_ylabel('\\n\\nvelocity')\n",
    "    ax.set_zlabel(r'$-V_\\mathrm{greedy}$', labelpad=12)\n",
    "    ax.view_init(50, -135)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd27dbefcdea8519c97880f13398718c",
     "grade": false,
     "grade_id": "cell-b77253bff3a33f07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.0001\n",
    "gamma = 1\n",
    "epsilon = 0.15\n",
    "nb_episodes = 300\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "state, _ = env.reset()\n",
    "norm_state = featurize(state)\n",
    "input_dim = len(norm_state[0])\n",
    "\n",
    "\n",
    "# define ANN topology\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dense(3, activation='linear'))\n",
    "\n",
    "opt = SGD(learning_rate=alpha)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "visited_states = []\n",
    "\n",
    "for j in tqdm(range(nb_episodes)):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e449cbba0bb676d8ee46fe76ee6ad7c5",
     "grade": false,
     "grade_id": "cell-78b56082c9fd71b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following cell to test the greedy-execution scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfbc69095269ede6e125e4d5f9ab39b5",
     "grade": false,
     "grade_id": "cell-53ef013dfafdb2d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"human\")\n",
    "state, _ = env.reset()\n",
    "\n",
    "k = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    k += 1\n",
    "    \n",
    "    norm_state = featurize(state)\n",
    "    action_values = np.squeeze(model1(norm_state).numpy())\n",
    "    action = np.argmax(action_values)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(k)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cdecbfe516b9889f9d25466732b0466",
     "grade": true,
     "grade_id": "cell-efd12b568890c007",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bd3b33653179d3a41a0a5139d40d75a",
     "grade": true,
     "grade_id": "cell-4c58a1f8901890d4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5937c35ec141e3ed61f84ecc3901513",
     "grade": false,
     "grade_id": "cell-e9b5438fa6bc895c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2) Feature Engineering in Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7177b1f9ab1e44ccea364f1f17db868f",
     "grade": false,
     "grade_id": "cell-89806f69ed38e223",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you may have seen, on-policy control is so much more complicated than prediction because some simple features as in task 1 will not always do the trick. A problem that can often be seen in cases like this originates from the generalization aspect of the ANN. Usually, generalization is an advantage, but we have to explore the state space sufficiently to enable its validity. In the MountainCar scenario, the state space can not be explored with simple means because reaching a specific state is always bound to preconditions. \n",
    "\n",
    "In order to overcome this generalization dilemma, a very different feature encoding can be employed. This can be done in the form of e.g. radial basis functions (RBFs), using RBF-feature \n",
    "$x_i = e^{-\\frac{||x - c_\\mathrm{i}||^2}{2 \\sigma_\\mathrm{i}^2}}$.\n",
    "\n",
    "For more information, you can read up in the Sutton and Barto (page 221 ff.).\n",
    "\n",
    "![](RBF.png)\n",
    "(Source: Reinforcement Learning, Sutton&Barto, p.221)\n",
    "\n",
    "With this encoding the feature vector will get very large, meaning that the dependency of the ANN output on single feature components will decrease. Contrary to just using the states as features, this will decrease the ANNs ability to extrapolate to unseen areas, because a very different set of features will be \"active\" within different regions of the state space. \n",
    "\n",
    "The following cell will prepare an RBF featurizer for the given state space. You should be able to use the same training algorithm as in task 1. (And if you happened to give up on task 1 before preparing a training algorithm, you now still have the chance to write one :P ).\n",
    "Show the learning curves (mean and standard deviation) as function of steps per episode! Therefore, evaluate multiple independent learning runs to retrieve the information required for plotting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e239b4be24c7e02fc27115f637b282bc",
     "grade": false,
     "grade_id": "cell-c8d13bb7239fd192",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "    (\"rbf0\", RBFSampler(gamma=5.0, n_components = 100)),\n",
    "    (\"rbf1\", RBFSampler(gamma=2.0, n_components = 100)),\n",
    "    (\"rbf2\", RBFSampler(gamma=1.0, n_components = 100)),\n",
    "    (\"rbf3\", RBFSampler(gamma=0.5, n_components = 100)),\n",
    "    ])\n",
    "featurizer.fit(scaler.transform(observation_examples))\n",
    "\n",
    "\n",
    "def featurize(state):\n",
    "    try:\n",
    "        scaled = scaler.transform([state])\n",
    "    except:\n",
    "        print(state)\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4d3a00c86ccf61f6077c47f85283e91",
     "grade": false,
     "grade_id": "cell-ee66b9d4e379a0aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Function to measure the performance using greedy execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe801f1af552affc1fe2179a029e259e",
     "grade": false,
     "grade_id": "cell-ce27249f89419a42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def performance_measure(measure_len = 20, epsilon=0):\n",
    "    k_vec = np.zeros(measure_len)\n",
    "    \n",
    "    for i in range(measure_len):\n",
    "        \n",
    "        k = 0\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            # env.render()\n",
    "            k += 1\n",
    "            norm_state = featurize(state)\n",
    "            action_values = np.squeeze(model2(norm_state).numpy())\n",
    "            \n",
    "            if epsilon < np.random.rand(1):\n",
    "                action = np.argmax(action_values)\n",
    "            else:\n",
    "                action = random.choice(range(3))\n",
    "                    \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "\n",
    "            if done:\n",
    "                k_vec[i] = k\n",
    "                break\n",
    "                \n",
    "    return np.mean(k_vec), np.std(k_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4657e8a9ed125052c00ecb447354041d",
     "grade": false,
     "grade_id": "cell-2256a05aec0f1dd8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.0001\n",
    "gamma = 1\n",
    "epsilon = 0.15\n",
    "nb_episodes = 100\n",
    "nb_perf_meas = 20\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "state, _ = env.reset()\n",
    "norm_state = featurize(state)\n",
    "input_dim = len(norm_state[0])\n",
    "\n",
    "\n",
    "performance_matrix = np.zeros((nb_episodes, nb_perf_meas))\n",
    "best_performance = np.zeros((nb_perf_meas,2))\n",
    "\n",
    "for m in tqdm(range(nb_perf_meas)):\n",
    "\n",
    "    # define ANN topology\n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model2.add(Dense(64, activation='relu'))\n",
    "    model2.add(Dense(3, activation='linear'))\n",
    "\n",
    "    opt = SGD(learning_rate=alpha)\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    visited_states = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cedf55b60baf928ce4d2609b2de98b43",
     "grade": false,
     "grade_id": "cell-415a7a9e835874a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Show the performance over the episodes of the 20 runs (average +- standard deviadtion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "460aaf7ec68c161d6e9cbbaa580d27f6",
     "grade": false,
     "grade_id": "cell-4d112c470be120af",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39bb45692732a0cb2eb714711c042b0f",
     "grade": true,
     "grade_id": "cell-a1f34cf2d43e050d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f7b0865068f8b154b0f91fcfb2f6cd8",
     "grade": false,
     "grade_id": "cell-ef71f78bcc9d8847",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55d936e9625f6b2627cbc9c352199586",
     "grade": false,
     "grade_id": "cell-694f99cdffeafbcd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode=\"human\")\n",
    "state, _ = env.reset()\n",
    "\n",
    "k = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    k += 1\n",
    "    norm_state = featurize(state)\n",
    "    action_values = np.squeeze(model2(norm_state).numpy())\n",
    "    action = np.argmax(action_values)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(k)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c4e411e1bef19b48f1231d02178ccc3",
     "grade": false,
     "grade_id": "cell-1acedea65cc82bde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3)  Feature Engineering in Linear Approximation with Online LSPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67e9dc0661c926c39c327f59be697d86",
     "grade": false,
     "grade_id": "cell-62038af01bfd788d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we have seen, the RBF feature engineering allows for good trainability of the ANN. Maybe, a similar result can even be achieved with the use of a linear function approximator if this feature encoding is reused. \n",
    "\n",
    "As we cannot encode the action itself with the same RBF method, we still needed to apply a change to the `featurize` function, such that the estimate can take the action properly into account. Please investigate the new `featurize` function and try to understand the motivation behind it.\n",
    "\n",
    "Write an RLS-SARSA algorithm to train a linear function approximator that estimates the action value and enables on-policy control. \n",
    "Show the learning curves (mean and standard deviation) as function of steps per episode. Therefore, evaluate multiple independent learning runs to retrieve the information required for plotting.\n",
    "Compare the two algorithms! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "964762f12430371785b2da4677105c2c",
     "grade": false,
     "grade_id": "cell-7bc7520ca4958ff3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3) Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "119bccbc1f8f520c4813c543593f5aae",
     "grade": false,
     "grade_id": "cell-c527672dc165b3ed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "    (\"rbf0\", RBFSampler(gamma=5.0, n_components = 100)),\n",
    "    (\"rbf1\", RBFSampler(gamma=2.0, n_components = 100)),\n",
    "    (\"rbf2\", RBFSampler(gamma=1.0, n_components = 100)),\n",
    "    (\"rbf3\", RBFSampler(gamma=0.5, n_components = 100)),\n",
    "    ])\n",
    "featurizer.fit(scaler.transform(observation_examples))\n",
    "\n",
    "\n",
    "def featurize(state, action):\n",
    "    action_vec = np.zeros([3, 1])\n",
    "    action_vec[action] = 1\n",
    "    \n",
    "    win = 0\n",
    "    if state[0] > 0.5:\n",
    "        win = 1\n",
    "    \n",
    "    try:\n",
    "        scaled = scaler.transform([state])\n",
    "    except:\n",
    "        print(state)\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    featurized = np.reshape(featurized, (-1, 1)) # make column vector\n",
    "    \n",
    "    featurized = np.append(featurized, np.array([[1]]), axis = 0)\n",
    "    \n",
    "    featurized_vec = np.array([])\n",
    "    featurized_vec = np.expand_dims(featurized_vec, axis=-1)\n",
    "    for a in action_vec:\n",
    "        if a == 1:\n",
    "            featurized_vec = np.append(featurized_vec, featurized, axis = 0)\n",
    "        elif a == 0:\n",
    "            featurized_vec = np.append(featurized_vec, np.zeros([len(featurized), 1]), axis = 0)        \n",
    "    \n",
    "    return featurized_vec * (1 - win)\n",
    "\n",
    "\n",
    "def policy(state, w, n, epsilon):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "    return feat_states[:, action], action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7487b3a221e411791deff39b5f2650e4",
     "grade": false,
     "grade_id": "cell-225e5fb37930c767",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_surface_LSPI(w, input_dim):\n",
    "    resolution = 100\n",
    "    pos_vec = np.linspace(-1.2, 0.6, resolution)\n",
    "    vel_vec = np.linspace(-0.07, 0.07, resolution)\n",
    "\n",
    "    pos_mat, vel_mat = np.meshgrid(pos_vec, vel_vec)\n",
    "    value_tensor = np.zeros([resolution, resolution])\n",
    "\n",
    "    for pos_idx, pos in enumerate(tqdm(pos_vec)):\n",
    "        for vel_idx, vel in enumerate(vel_vec):\n",
    "            feat_state, _ =  policy(np.array([pos, vel]), w, env.action_space.n, 0)\n",
    "            value_tensor[vel_idx, pos_idx] = np.transpose(feat_state) @ w\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.subplots()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(pos_mat, vel_mat, -value_tensor, cmap=\"viridis\")\n",
    "    ax.set_xlabel('\\n\\nposition')\n",
    "    ax.set_ylabel('\\n\\nvelocity')\n",
    "    ax.set_zlabel(r'$-V_\\mathrm{greedy}$', labelpad=12)\n",
    "    ax.view_init(50, -135)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measure_LSPI(measure_len=20, epsilon=0):\n",
    "    k_vec = np.zeros(measure_len)\n",
    "\n",
    "    for i in range(measure_len):\n",
    "\n",
    "        k = 0\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # env.render()\n",
    "            k += 1\n",
    "            _, action = policy(state, w_policy, env.action_space.n, epsilon)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if done:\n",
    "                k_vec[i] = k\n",
    "                break\n",
    "\n",
    "    return np.mean(k_vec), np.std(k_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f24f301facf9ef5e1f00e25fdd0d66db",
     "grade": false,
     "grade_id": "cell-805a7b20e5c19cc2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "# try lambda < 1, what do you see?\n",
    "_lambda = 1 # we call it like that because lambda is a defined command in python\n",
    "nb_episodes = 100\n",
    "epsilon = 0.15\n",
    "k_w = 10\n",
    "nb_perf_meas = 20\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "state, _ = env.reset()\n",
    "feat_state = featurize(state, 0)\n",
    "feat_dims = len(feat_state)\n",
    "\n",
    "performance_matrix_ = np.zeros((nb_episodes, nb_perf_meas))\n",
    "best_performance = np.zeros((nb_perf_meas,2))\n",
    "\n",
    "for m in tqdm(range(nb_perf_meas)):\n",
    "    k = 0\n",
    "    P = np.eye(feat_dims)\n",
    "    w = np.zeros(feat_dims)\n",
    "    w = np.expand_dims(w, axis=-1)\n",
    "\n",
    "    w_policy = np.copy(w)\n",
    "\n",
    "    for j in tqdm(range(nb_episodes)):\n",
    "        \n",
    "        length = 0\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        feat_state, action = policy(state, w, env.action_space.n, epsilon)\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            #env.render()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            feat_next_state, next_action = policy(next_state, w_policy, env.action_space.n, epsilon)\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # time propagation\n",
    "            feat_state = feat_next_state\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "            k += 1\n",
    "            length += 1\n",
    "\n",
    "            if done:\n",
    "                performance_matrix_[j,m] = length\n",
    "                #print(f\"Episode: {j}, Length {length}\")\n",
    "                if j % 10 == 0:\n",
    "                    plot_surface_LSPI(w, feat_dims)\n",
    "                break\n",
    "                \n",
    "    best_performance[m][0], _ = performance_measure_LSPI(epsilon=0)\n",
    "    best_performance[m][1], _ = performance_measure_LSPI(epsilon=0.15)\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5b5db62de3bab34696289d850549c70",
     "grade": false,
     "grade_id": "cell-c20d5a0acfbe704f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('MountainCar-v0', render_mode=\"human\")\n",
    "state , _= env.reset()\n",
    "\n",
    "k = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    k += 1\n",
    "    \n",
    "    _, action = policy(state, w_policy, env.action_space.n, 0)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(k)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d462312e442e4d170e1373bf7314e1b2",
     "grade": false,
     "grade_id": "cell-f07b3d6e621dab52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Show the performance over the episodes of the 20 runs (average +- standard deviadtion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "172dac770bbfa73323ab371c9a7fb6c3",
     "grade": false,
     "grade_id": "cell-f67a6f028da142ec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6eedea930e2802d854d2fd96af31552f",
     "grade": true,
     "grade_id": "cell-44bdf8ebb9fcd19f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ac3e6271b5c34c39a232054d93c1f96",
     "grade": false,
     "grade_id": "cell-6e81fef7350a76be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6aef6cbe20b869d208b2ee0489a7544d",
     "grade": true,
     "grade_id": "cell-505c610e85afc61c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
