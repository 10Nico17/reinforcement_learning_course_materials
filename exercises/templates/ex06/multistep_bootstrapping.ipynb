{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a2b1e08cb97184c97d7470c3e4b17a4",
     "grade": false,
     "grade_id": "cell-c833698b7dad927d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 6: Multi-Step Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee600d21804633c09a523a6d31acbd26",
     "grade": false,
     "grade_id": "cell-7cf627dacfec200a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise we will have a look at n-step methods and eligibility trace. The n-step methods are a class of reinforcement learning algorithms that are an abstraction of the Monte Carlo and TD(0) methods discussed earlier and include them as special cases. Furthermore, we also consider the eligibility traces, which take a reverse approach to determining the state values. The environment we will be dealing with is a little more typical for control research: the inverted pendulum. \n",
    "\n",
    "![](https://miro.medium.com/max/1000/1*TNo3x9zDi1lVOH_3ncG7Aw.gif)\n",
    "\n",
    "To implement this environment, we will make use of the gymnasium library. Please install the gymnasium library within your preferred Python environment using:\n",
    "\n",
    "```pip install gymnasium```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e29d72a7366ca91e624ea3243bbb16f6",
     "grade": false,
     "grade_id": "cell-68ba542456c544d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50a139d58ebe2c067271409d4d4635bd",
     "grade": false,
     "grade_id": "cell-b9853bfaec1d8013",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check if the installation and import work by executing the following cell. A window with an animation of the pendulum should open, display some random actions, and close automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22bb888f7e3d23eff0647b95b584f75b",
     "grade": false,
     "grade_id": "cell-8e133fbe2615fd5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "env = env.unwrapped # removes a built-in time limit of k_T = 200, we want to determine the time limit ourselves\n",
    "\n",
    "state, _ = env.reset()\n",
    "for _ in range(300):\n",
    "    env.render()\n",
    "    state, reward, terminated, _, _ = env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bffeb64e00c1b54db5b127f6457215e0",
     "grade": false,
     "grade_id": "cell-f29246bdc3e421c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The goal of this environment is to bring the pendulum into the upper neutral position, where the angle $\\theta = 0$ and the angular velocitiy $\\frac{\\text{d}}{\\text{d}t}\\theta=\\omega=0$. The reward function is already designed that way and does not need further specification. For further information about the environment you may refer to the code and documentation of Farama Foundation's `gymnasium`:\n",
    "\n",
    "[Documentation of the gymnasium pendulum](https://gymnasium.farama.org/environments/classic_control/pendulum/)\n",
    "\n",
    "[Pendulum environment in the gymnasium Github repository](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/pendulum.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2a638f16d68f332fc0f3a82e032b73f",
     "grade": false,
     "grade_id": "cell-8570b84cc28ffc4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Discretization of Action and State Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e897657ed015c5ad2d6a3f9f1176d858",
     "grade": false,
     "grade_id": "cell-ee59383187979c94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Unlike the racetrack environment, the inverted pendulum comes with a continuous action and state space. Although it is possible to handle systems with these characteristics, we did not yet learn how to deal with them. For now, we only know how to implement agents for discrete action and state spaces. Accordingly, we will also try to represent the inverted pendulum within a discrete state / action space. For this, a discretization is necessary.\n",
    "\n",
    "The pendulum has three state variables relating to the momentary angular position $\\theta$:\n",
    "\\begin{align*}\n",
    "    x=\\begin{bmatrix}\n",
    "    \\text{cos}(\\theta)\\\\\n",
    "    \\text{sin}(\\theta)\\\\\n",
    "    \\frac{\\text{d}}{\\text{d}t}\\theta\n",
    "    \\end{bmatrix}\n",
    "    \\in\n",
    "    \\begin{bmatrix}\n",
    "    [-1, 1]\\\\\n",
    "    [-1, 1]\\\\\n",
    "    [-8 \\, \\frac{1}{\\text{s}}, 8 \\, \\frac{1}{\\text{s}}]\n",
    "    \\end{bmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "and one input variable which relates to the torque applied at the axis of rotation:\n",
    "\n",
    "$u = T \\in [-2 \\, \\text{N}\\cdot\\text{m}, 2 \\, \\text{N}\\cdot\\text{m}]$\n",
    "\n",
    "After the discretization, we want the system to be defined on sets of non-negative natural numbers:\n",
    "\n",
    "\\begin{align*}\n",
    "    x_d =\n",
    "    \\text{discretize_state}(x)\n",
    "    \\in\n",
    "    \\begin{bmatrix}\n",
    "    \\{0,1,2,...,d_{\\theta}-1\\}\\\\\n",
    "    \\{0,1,2,...,d_{\\theta}-1\\}\\\\\n",
    "    \\{0,1,2,...,d_{\\omega}-1\\}\n",
    "    \\end{bmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "$\n",
    "u_d =\n",
    "\\text{discretize_action}(u)\n",
    "\\in\n",
    "\\{0,1,2,...,d_{T}-1\\}.\n",
    "$\n",
    "\n",
    "Since action is selected within the discrete action space, we need to transform it accordingly:\n",
    "\n",
    "$\n",
    "u=\n",
    "\\text{continualize_action}(u_d):\n",
    "\\{0,1,2,...,d_{T}-1\\} \\rightarrow [-2 \\, \\text{N}\\cdot\\text{m}, 2 \\, \\text{N}\\cdot\\text{m}]\n",
    ".\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e11aa2bd2d560f6080a639847c3da91c",
     "grade": false,
     "grade_id": "cell-28b6b992373b4a65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write the functions `discretize_state` and `continualize_action`, such that a discrete RL agent can be applied. (Please note that all I/O of `gymnasium` consists of numpy arrays.) Write the functions in such a way that the number of discretization intervals $d_\\theta, d_\\omega, d_T$ are parameters that can be changed for different tests. The discretization intervals should be uniformly distributed on their respective state space.\n",
    "\n",
    "A parametrization of $d_\\theta = d_\\omega = d_T = 15$ can be used to yield satisfactory results in this exercise.\n",
    "However, does it make a difference if the number of discretization intervals is odd or even? If yes, what should be preferred for the given environment? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "786fa6ead07a16cc0540a14e9cbaf99a",
     "grade": false,
     "grade_id": "cell-cfd4a4f7a22ce35c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Solution 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b63df0573df481b3efcf661e77df3be",
     "grade": true,
     "grade_id": "cell-cf67ba4807c7ce8c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ecab1aa3f9a898eb3d8719cae1072e7",
     "grade": false,
     "grade_id": "cell-af38d4d166803785",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "d_T = 15\n",
    "d_theta = 15\n",
    "d_omega = 15\n",
    "\n",
    "\n",
    "def discretize_state(states):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def continualize_action(disc_action):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04823d55c0a413ce03bb36f6ccc0ed79",
     "grade": false,
     "grade_id": "cell-54296429c6a25f98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following cell for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ed7cb079ebb6b4ed782c528b1dcfa76",
     "grade": false,
     "grade_id": "cell-755b0b9277910870",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "state, _ = env.reset()\n",
    "for _ in range(5):\n",
    "    disc_action = np.random.choice(range(9))\n",
    "    cont_action = continualize_action(disc_action)\n",
    "    print(\"discrete action: {}, continuous action: {}\".format(disc_action, cont_action))\n",
    "    \n",
    "    state, reward, terminated, _, _ = env.step(cont_action) # take a random action\n",
    "    disc_state = discretize_state(state)\n",
    "    print(\"discrete state: {}, continuous state: {}\".format(disc_state, state))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49a4d732185a7169c64e97cb97873700",
     "grade": true,
     "grade_id": "cell-7050729ab9b288bc",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bfbde17bf71060103fa2eb7c9a140d7",
     "grade": true,
     "grade_id": "cell-191ecd76d4787fb5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70213d8b39435fe626032697aadb5f19",
     "grade": false,
     "grade_id": "cell-2575b2d2065717a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) n-Step Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "378e6a1c5e0d6e2fcf84910c6ec437fc",
     "grade": false,
     "grade_id": "cell-71c349849a7bdad7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write an on-policy n-step Sarsa control algorithm for the inverted pendulum from scratch. This time, no code template is given. \n",
    "\n",
    "Use the following parameters: $\\alpha=0.1, \\gamma=0.9, \\varepsilon=0.1, n=10$ with 500 time steps in 2000 episodes.\n",
    "\n",
    "![](nStepSARSA_Algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a42894ad9d9ad093e424f9aad26e362",
     "grade": true,
     "grade_id": "cell-877e2e0ac6a7510e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14a89ae595d496a3453a838e2f27974d",
     "grade": false,
     "grade_id": "cell-37c9e8a6c5268048",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1') # , render_mode=\"human\"\n",
    "env = env.unwrapped\n",
    "\n",
    "alpha = 0.1  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "epsilon = 0.1  # epsilon greedy parameter\n",
    "n = 10  # steps between updates\n",
    "\n",
    "nb_episodes = 2000  # number of episodes\n",
    "nb_steps = 500  # length of episodes\n",
    "\n",
    "action_values = np.zeros([d_theta, d_theta, d_omega, d_T])\n",
    "# int is necessary for indexing\n",
    "pi = np.zeros([d_theta, d_theta, d_omega], dtype=int)\n",
    "\n",
    "# we can use this to figure out how well the learning worked\n",
    "cumulative_reward_history = []\n",
    "\n",
    "for j in tqdm(range(nb_episodes), position=0, leave=True):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "pi_learned = np.copy(pi)  # save pi in cache under different name for later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a70529957ec84e9354815ebb1a42cf3b",
     "grade": true,
     "grade_id": "cell-fc8a6d2fe73f71bf",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "310528718879e1d68cd72cc27c699a51",
     "grade": false,
     "grade_id": "cell-ddebe8848a817b91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Greedy Execution\n",
    "\n",
    "Test the learned policy by pure greedy execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c18c37c1548cdee36cd81f336479ead",
     "grade": false,
     "grade_id": "cell-6ffa29bb63e9fc42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "env = env.unwrapped\n",
    "\n",
    "nb_steps = 200\n",
    "\n",
    "state, _ = env.reset() # initialize x_0\n",
    "disc_state = tuple(discretize_state(state)) # use tuple indexing\n",
    "disc_action = pi_learned[disc_state]\n",
    "\n",
    "for k in range(nb_steps):\n",
    "        \n",
    "    cont_action = continualize_action(disc_action)\n",
    "    env.render() # comment out for faster execution\n",
    "    state, reward, terminated, _, _ = env.step(cont_action)\n",
    "    disc_state = tuple(discretize_state(state))\n",
    "        \n",
    "    if terminated:\n",
    "        break\n",
    "        \n",
    "    disc_action = pi_learned[disc_state] # exploitative action\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24441f971fab220f1b9dc439154548ac",
     "grade": false,
     "grade_id": "cell-4731b891d975389f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 3) Recursive updates: TD($\\lambda$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94491f1d2bbf8eac5d058c16c849563a",
     "grade": false,
     "grade_id": "cell-24a77a257e20058a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Both, $n$-step and $\\lambda$-return updates, are based on a forward view. That means we have to wait for future states and rewards before an update can be performed.\n",
    "We therefore introduce an eligibility traces, which follows the general idea that previous actions have significantly led to the current situation. Contrary to n-step learning, however, intuition tells us that more recent decisions had a more severe impact on the present situation than decisions that were made a long time ago. Thus, it may be helpful to integrate a forgetting factor $\\lambda$ which decreases the assumed influence of actions over time.\n",
    "\n",
    "Solution 2 is now to be extended by eligibility traces $z_k(x_k)$ within the action-value update. Test it for different values of $\\lambda$. How sensitive is the process to the choice of $\\lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a59288d5637849cb70450e84a6fcec6e",
     "grade": false,
     "grade_id": "cell-8872e02f8c41136a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1') # , render_mode=\"human\"\n",
    "env = env.unwrapped\n",
    "\n",
    "alpha = 0.1  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "epsilon = 0.1  # epsilon greedy parameter\n",
    "lamb = 0.8  # forgetting factor\n",
    "\n",
    "nb_episodes = 5000  # number of episodes\n",
    "nb_steps = 500  # length of episodes\n",
    "\n",
    "action_values = np.zeros([d_theta, d_theta, d_omega, d_T])\n",
    "# init eligibility trace\n",
    "eligibility = np.zeros([d_theta, d_theta, d_omega])\n",
    "# int is necessary for indexing\n",
    "pi = np.zeros([d_theta, d_theta, d_omega], dtype=int)\n",
    "\n",
    "# we can use this to figure out how well the learning worked\n",
    "cumulative_reward_history = []\n",
    "\n",
    "for j in tqdm(range(nb_episodes), position=0, leave=True):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cebb6a4569e08179035356d594eb08e1",
     "grade": false,
     "grade_id": "cell-05f58d9e3aabc348",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(cumulative_reward_history)\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(r\"$\\sum R$\")\n",
    "plt.show()\n",
    "\n",
    "print(np.shape(cumulative_reward_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e27e6b25ace24ac134a93cfb0c6e00a1",
     "grade": false,
     "grade_id": "cell-ea25c02b87a6e9b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Greedy Execution\n",
    "\n",
    "Test the learned policy by pure greedy execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa7321acf996dc629d2907ca049555c7",
     "grade": false,
     "grade_id": "cell-7272b1acfbd4b325",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "env = env.unwrapped\n",
    "\n",
    "nb_steps = 200\n",
    "\n",
    "state, _ = env.reset() # initialize x_0\n",
    "disc_state = tuple(discretize_state(state)) # use tuple indexing\n",
    "disc_action = pi[disc_state]\n",
    "\n",
    "for k in range(nb_steps):\n",
    "        \n",
    "    cont_action = continualize_action(disc_action)\n",
    "    env.render() # comment out for faster execution\n",
    "    state, reward, terminated, _, _ = env.step(cont_action)\n",
    "    disc_state = tuple(discretize_state(state))\n",
    "        \n",
    "    if terminated:\n",
    "        break\n",
    "        \n",
    "    disc_action = pi[disc_state] # exploitative action\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5bc5021ebafe380ecbaafaf265c22e8",
     "grade": true,
     "grade_id": "cell-035605a2ad99cdb5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e2b9d9f90b36420b94e4dfed37873a9",
     "grade": true,
     "grade_id": "cell-922b5151274ba2ff",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "862e7148e1e62f3474c003d2474f9ed3",
     "grade": true,
     "grade_id": "cell-473370f06b9331b3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb2219f24995afd38fc5a84c109333eb",
     "grade": true,
     "grade_id": "cell-76040c4aac8f51f9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
