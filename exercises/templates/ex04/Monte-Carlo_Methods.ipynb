{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b42fbb0ba1a6c95e93b871ed850189de",
     "grade": false,
     "grade_id": "cell-9dea5c81cd34f5a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 04): Monte-Carlo Methods\n",
    "\n",
    "In this exercise we make use of the racetrack environment (racetrack_environment.py) to test Monte-Carlo methods.\n",
    "\n",
    "For the start, please execute the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eafb076bda5565610309e2eef714fd96",
     "grade": false,
     "grade_id": "cell-9c8cfa434031df78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from racetrack_environment import RaceTrackEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e336a266b7deb26c94551a8fc603a62",
     "grade": false,
     "grade_id": "cell-46112ad628791ed0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Execute the follwoing cell to built a race track using the `RaceTrackEnv` as a test scenario.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f09f31f01a0073294c8e625b3a7c4191",
     "grade": false,
     "grade_id": "cell-ab28c0c5fbe2404e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Build the course\n",
    "_course_dim = (8, 10)\n",
    "_inner_wall_dim = (2, 6)\n",
    "\n",
    "def build_uturn_course(course_dim, inner_wall_dim):\n",
    "    \"\"\"\n",
    "    Build a race track for the u-turn street scenario.\n",
    "    Start and finish line are placed in the center top and bottom respectively. The course dimension specifications\n",
    "    do not consider a bounding wall around the track, which is inserted additionally. \n",
    "\n",
    "    \"\"\"\n",
    "    track = []\n",
    "    wall_up_bound = course_dim[0]//2 - inner_wall_dim[0] // 2\n",
    "    wall_bottom_bound = course_dim[0]//2 + inner_wall_dim[0]//2\n",
    "    street_width = course_dim[1]//2 - inner_wall_dim[1]//2\n",
    "    # construct course line by line\n",
    "    for i in range(course_dim[0]):\n",
    "        if i < wall_up_bound:\n",
    "            half_street_len = course_dim[1]//2 - 1\n",
    "            track_row = 'W'*(half_street_len//2+1) + 'W-' + 'o'*(half_street_len-1+half_street_len//2)\n",
    "        elif  wall_up_bound <= i < wall_bottom_bound:\n",
    "            track_row = 'W'*street_width + 'W'*inner_wall_dim[1] + 'o'*street_width\n",
    "        else:\n",
    "            track_row = 'W'*(half_street_len//2+1) + 'W+' + 'o'*(half_street_len-1+half_street_len//2)\n",
    "        track.append(track_row)\n",
    "    # add boundary\n",
    "    track = ['W'*course_dim[1]] + track + ['W'*course_dim[1]]\n",
    "    track = ['W'+s+'W' for s in track]\n",
    "    return track\n",
    "    \n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "for row in course:\n",
    "    print(row)\n",
    "    \n",
    "pos_map =  track.course  # overlay track course\n",
    "plt.imshow(pos_map, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b75600d3f638376ef250ff78b7f9ff4",
     "grade": false,
     "grade_id": "cell-ce1387b114d55595",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Monte-Carlo-Based Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a61a092f27ad32f67ab4e111e4838e0",
     "grade": false,
     "grade_id": "cell-a3672043edcf93af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a first-visit Monte-Carlo algorithm to evaluate the dummy policy as defined below on the U-turn course. The dummy policy turns the car to the right as soon as it stands in front of a wall. Try to understand how the policy works before you start to code. Actions (accelerations in given directions) are encoded according to the following diagram:\n",
    "\n",
    "![](Directions_Legend.png)\n",
    "\n",
    "How can we interprete the state values resulting from the evaluation with first-visit Monte-Carlo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb9337e88dd10e251e1105c85cf20d66",
     "grade": false,
     "grade_id": "cell-32d1e89b52d7ea2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb230582893bd38f28bd7d04a0e422ae",
     "grade": false,
     "grade_id": "cell-ea22080ba0fc3ad7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Algorithm given below.\n",
    "\n",
    "The simple and deterministic dummy policy will always guarantee the car to reach the finish line. Thus, the state values can be interpreted as the number of timesteps that is necessary to reach the goal from that specific state (i.e. position and velocity) if we are following the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e919458c9c1411d0ee6ac1c2d534bfd",
     "grade": false,
     "grade_id": "cell-296f673d66265e7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Select course and initialize dummy policy\n",
    "\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "dummy_slow_pi = np.ones([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY]) * 4 \n",
    "\n",
    "dummy_slow_pi[:track.bounds[0]//2, :, 0 , 0] = 5   # go right\n",
    "dummy_slow_pi[:track.bounds[0]//2, -2:, 0 , :] = 6 # go bottom left\n",
    "dummy_slow_pi[-2:, track.bounds[1]//2:, : , 0] = 0 # go top left\n",
    "\n",
    "pi = dummy_slow_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b7af3ae44f4d16996a91df5e438d421",
     "grade": false,
     "grade_id": "cell-ac5467fab5f148f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize the value function\n",
    "values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY])\n",
    "\n",
    "# initialize an empty dict to count the number of visits\n",
    "n_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 500 # number of evaluated episodes\n",
    "no_steps = 2000 # number of allowed timesteps per episode\n",
    "\n",
    "for e in tqdm(range(no_episodes), position=0, leave=True):\n",
    "    \n",
    "    # initialize variables in which collected data will be stored\n",
    "    states = [] # list of tuples\n",
    "    rewards = [] # list of floats\n",
    "    visited_states = set() # set of tuples\n",
    "    first_visit_list = [] # list of booleans\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "950030ffd11e464f008f618a8b94e834",
     "grade": false,
     "grade_id": "cell-6fe53fdd68a6c909",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To visualize the result of the evaluation, plot the state values as a function of **position only** (so that you get a two dimensional representation of the state value) and in the form of a tabular represenation and a heatmap. In order to omit dependence of the velocity dimensions, use the minimum of the value function with respect to the velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "825ce21dcb9c2a4de85dc92fe3d652f3",
     "grade": false,
     "grade_id": "cell-74fc6bcd5def8261",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def text_print_pos_map(_pos_map):\n",
    "    for row in _pos_map:\n",
    "        print(' '.join(x_size*['{}']).format(*[str(int(r)).zfill(3) for r in row]))\n",
    "        \n",
    "def plot_pos_map(_pos_map):\n",
    "    plt.imshow(_pos_map, cmap='hot', interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "# calculate minimum value with respect to velocities\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "pos_map = np.zeros((y_size, x_size))\n",
    "\n",
    "for s_x in range(x_size):\n",
    "    for s_y in range(y_size):\n",
    "        pos_map[s_y, s_x] = np.min(values[s_y, s_x, :, :])\n",
    "        \n",
    "text_print_pos_map(pos_map)\n",
    "plot_pos_map(-pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb263ef6d6bf6e83fccc3c6c57d56393",
     "grade": false,
     "grade_id": "cell-54642e38ce9d8a67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) On-Policy $\\varepsilon$-Greedy Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12ab33abc90a0c78c45f40f7bb2b57df",
     "grade": false,
     "grade_id": "cell-a81f379107be8dd3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Starting with the previously used turn-right-if-wall dummy policy, write an on-policy Monte-Carlo based first-visit $\\varepsilon$-greedy control algorithm to solve the U-turn course. The policy is now stochastic: it does not contain simple action commands for each state, but probabilities for each possible action. Again, please make sure to understand how the stochastic policy works before coding.\n",
    "\n",
    "\n",
    "Make sure to implement an upper bound for episode length (we suggest a boundary of 200 steps). Why do we need a bound like this? What happens to the state values / state-action values if we increase the bound?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42a41a387c73bb153c4b4dd65ef68c4f",
     "grade": false,
     "grade_id": "cell-2143fc4c280b5b6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af0fd3b2294022f8397d32ab9ccdbbb5",
     "grade": true,
     "grade_id": "cell-89a131cffdbb5d52",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "817c14229f5bb2d758ddff8744d6d878",
     "grade": false,
     "grade_id": "cell-b686db0a0a7aed59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# dummy policy\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "\n",
    "dummy_slow_stoch_pi = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 9])\n",
    "\n",
    "dummy_slow_stoch_pi[  :,   :, :, :, 4] = 1 # set probability of doing nothing to one for every state\n",
    "\n",
    "# set probability to go right:\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 5] = 1 \n",
    "# set probability to do nothing where we want to go right:\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 4] = 0 \n",
    "\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 6] = 1 # probability to go bottom left\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 4] = 0 \n",
    "\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 0] = 1 # probability to go top left\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 4] = 0 \n",
    "\n",
    "pi = dummy_slow_stoch_pi                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38ea49f80de410099be46bae73278807",
     "grade": true,
     "grade_id": "cell-9568aa87f2614759",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize action_values and counting dict\n",
    "action_values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 3, 3])\n",
    "n_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "epsilon = 0.1 # exploration probability\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 5000 # number of evaluated episodes\n",
    "no_steps = 200 # number of evaluated timesteps per episode\n",
    "\n",
    "\n",
    "track = RaceTrackEnv(course)\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "\n",
    "for e in tqdm(range(no_episodes), desc='episode', mininterval=2):\n",
    "      \n",
    "    # initialize variables in which collected data will be stored\n",
    "    action_states = [] # list of tuples\n",
    "    rewards = [] # list of floats\n",
    "    visited_action_states = set() # set of tuples\n",
    "    first_visit_list = [] # list of booleans\n",
    "    \n",
    "    pos_map = np.zeros((y_size, x_size)) # initializes a map that can be plotted\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # this code fragment is to plot the sampled map, comment out for faster computation\n",
    "    print('Sample trajectory on learned policy in episode {}:'.format(e))\n",
    "    pos_map = (pos_map > 0).astype(np.float32)\n",
    "    pos_map +=  track.course  # overlay track course\n",
    "    plot_pos_map(pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f80985713e5421d2a2f419b230c62e5d",
     "grade": false,
     "grade_id": "cell-66a45f80f155ca39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the code block directly below to test the resulting deterministic greedy policy (several samples are taken in order to show behavior in all different starting positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6b9861efffd8ce5d9178f15c2c87acb",
     "grade": false,
     "grade_id": "cell-ba1f0a2326526aeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "no_episodes = 10\n",
    "for e in range(no_episodes):\n",
    "    \n",
    "    pos_map = np.zeros((y_size, x_size))\n",
    "    p, v = track.reset()\n",
    "    for k in range(200):\n",
    "        s_y, s_x = p[0], p[1]\n",
    "        s_vy, s_vx = v[0], v[1]\n",
    "        \n",
    "        pos_map[s_y, s_x] += 1  # exploration map\n",
    "        \n",
    "        action = np.argmax(pi[s_y, s_x, s_vy, s_vx])\n",
    "        a = track.action_to_tuple(action)\n",
    "        action_state = track.state_action((p, v), a)\n",
    "\n",
    "        (p, v), reward, done, _ = track.step(a)\n",
    "\n",
    "        if done:\n",
    "            break \n",
    "\n",
    "    print('Sample trajectory on learned policy in episode {}:'.format(e))\n",
    "    pos_map = (pos_map > 0).astype(np.int16)\n",
    "    pos_map +=  track.course  # overlay track course\n",
    "    plot_pos_map(pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef63a401fd3977bb15d23b78ac8ff7f0",
     "grade": false,
     "grade_id": "cell-679b71dfdf0742d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Off-Policy $\\varepsilon$-Greedy Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a69ecda6b4e16a2db5f3e7fa43d25048",
     "grade": false,
     "grade_id": "cell-c026747de70d5b31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using the dummy-policy from 2) as a behavior policy, write an off-policy Monte-Carlo algorithm with weighted importance sampling.\n",
    "\n",
    "Has the result gotten better or worse? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2532e83b0832734af34182e8f8497bbf",
     "grade": false,
     "grade_id": "cell-2620db37ac7ac47d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eef934924ccc6b5b6a252ea062feed79",
     "grade": true,
     "grade_id": "cell-2ac7d4c49a011abb",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5344ca769b91fad52abbedca36d6a933",
     "grade": false,
     "grade_id": "cell-6f679149b605b3a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Dummy Policy\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "dummy_slow_stoch_pi = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 9])\n",
    "\n",
    "# as the behavior policy is not alternated, there is no possibility to implement the epsilon parameter later\n",
    "# hence, we need to implemented it right here\n",
    "epsilon = 0.1\n",
    "\n",
    "dummy_slow_stoch_pi[  :,   :, :, :, 4] = 1 - epsilon + epsilon / 9\n",
    "for i in range(9):\n",
    "    if i != 4:\n",
    "        dummy_slow_stoch_pi[  :, :, :, :, i] = epsilon / 9\n",
    "    \n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 5] = 1-epsilon + epsilon/9\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 4] = epsilon / 9\n",
    "\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 6] = 1-epsilon + epsilon/9\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 4] = epsilon / 9\n",
    "\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 0] = 1-epsilon + epsilon/9\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 4] = epsilon / 9\n",
    "\n",
    "behavior_policy = dummy_slow_stoch_pi  \n",
    "\n",
    "pi = np.copy(behavior_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36c985b9f61370655c995478e57a93c3",
     "grade": true,
     "grade_id": "cell-a7dfd6b6d5a875bd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize action_values and dict of cumulated WIS weights\n",
    "action_values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 3, 3])\n",
    "c_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "# epsilon = 0.1 was defined within the behavior policy\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 1000 # number of evaluated episodes\n",
    "no_steps = 200 # number of evaluated timesteps per episode\n",
    "\n",
    "course = course\n",
    "track = RaceTrackEnv(course)\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "\n",
    "for e in tqdm(range(no_episodes), desc='episode', mininterval=2):\n",
    "    \n",
    "    action_states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    pos_map = np.zeros((y_size, x_size))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # code fragment for plotting      \n",
    "    pos_map = (pos_map > 0).astype(np.float32)\n",
    "    pos_map +=  track.course  # overlay track course\n",
    "    print('Sample trajectory on learned policy in episode {}:'.format(e))\n",
    "    plot_pos_map(pos_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6b460f156114bd9e76e301c47e564d2",
     "grade": false,
     "grade_id": "cell-e3e001eb425a07bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "for e in range(episodes):\n",
    "    \n",
    "    pos_map = np.zeros((y_size, x_size))\n",
    "    p, v = track.reset()\n",
    "    for k in range(200):\n",
    "        s_y, s_x = p[0], p[1]\n",
    "        s_vy, s_vx = v[0], v[1]\n",
    "        \n",
    "        pos_map[s_y, s_x] += 1  # exploration map\n",
    "        \n",
    "        action = np.argmax(pi[s_y, s_x, s_vy, s_vx])\n",
    "        \n",
    "        a = track.action_to_tuple(action)\n",
    "        action_state = track.state_action((p, v), a)\n",
    "\n",
    "        (p, v), reward, done, _ = track.step(a)\n",
    "\n",
    "        if done:\n",
    "            print('Done')\n",
    "            break \n",
    "\n",
    "            \n",
    "    print('Sample trajectory on learned policy in episode {}:'.format(e))\n",
    "    pos_map = (pos_map > 0).astype(np.int16)\n",
    "    pos_map +=  track.course  # overlay track course\n",
    "    plot_pos_map(pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "454e031d1c59728e3e90817c0b6ee5fe",
     "grade": false,
     "grade_id": "cell-75a92b1a891b9346",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4) Extra Challenge: A More Complex Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0643d7c199188fe7341ccfb485b235c",
     "grade": false,
     "grade_id": "cell-9eb7640363641603",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The course given below poses a substantially harder challenge for Monte-Carlo based algorithms. Why? If you want to try solving it yourself, be aware that it may take much longer until a successful policy is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29f8d5a316596da9a4a508934f234191",
     "grade": false,
     "grade_id": "cell-7fdf744535830e4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Build the course\n",
    "_course_dim = (8, 10)\n",
    "_inner_wall_dim = (2, 6)\n",
    "\n",
    "def build_rect_course(course_dim, inner_wall_dim):\n",
    "    \"\"\"\n",
    "    Build a race track given specifications for the outer cyclic street and inner wall dimensions.\n",
    "    Start and finish line should be placed in the center top. The course dimension specifications\n",
    "    do not consider a bounding wall around the track, which must be inserted additionally.\n",
    "    \n",
    "    Args:\n",
    "        course_dim: 2-tuple, (y-dim, x-dim): The size of the track without outer walls.\n",
    "        inner_wall_dim: 2-tuple (y-dim, x-dim): The size of the inner wall\n",
    "    \n",
    "    \"\"\"\n",
    "    track = []\n",
    "    wall_up_bound = course_dim[0]//2 - inner_wall_dim[0] // 2\n",
    "    wall_bottom_bound = course_dim[0]//2 + inner_wall_dim[0]//2\n",
    "    street_width = course_dim[1]//2 - inner_wall_dim[1]//2\n",
    "    # construct course line by line\n",
    "    for i in range(course_dim[0]):\n",
    "        if i < wall_up_bound:\n",
    "            half_street_len = course_dim[1]//2 - 1\n",
    "            track_row = 'o'*half_street_len + '+W-' + 'o'*(half_street_len-1)\n",
    "        elif  wall_up_bound <= i < wall_bottom_bound:\n",
    "            track_row = 'o'*street_width + 'W'*inner_wall_dim[1] + 'o'*street_width\n",
    "        else:\n",
    "            track_row = 'o'*course_dim[1]\n",
    "        track.append(track_row)\n",
    "    # add boundary\n",
    "    track = ['W'*course_dim[1]] + track + ['W'*course_dim[1]]\n",
    "    track = ['W'+s+'W' for s in track]\n",
    "    return track\n",
    "    \n",
    "course = build_rect_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "for row in course:\n",
    "    print(row)\n",
    "    \n",
    "pos_map =  track.course  # overlay track course\n",
    "plot_pos_map(pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa7b1016142898eb25e6fe46aa04cde5",
     "grade": false,
     "grade_id": "cell-6382c23e5d25c036",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fb85e183f7f07042f3d34a11c522118",
     "grade": true,
     "grade_id": "cell-15500169957f16d3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8018e734bbb8831a855f6b55bcd4339c",
     "grade": true,
     "grade_id": "cell-9f170e15782def02",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfb0d3029afb856c4ceaa562d55b7c10",
     "grade": true,
     "grade_id": "cell-feb4a6303f6ae334",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
