{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa3b7b927a4c3704f7eff76b898076f1",
     "grade": false,
     "grade_id": "cell-d485c91a8c052319",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 8: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76d2a0e8ba3b8b2a3ade4386fc1dbb6c",
     "grade": false,
     "grade_id": "cell-ef5b12a5b0a9750d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise a few basic supervised learning pipelines are elaborated.\n",
    "* Data analysis\n",
    "* Feature engineering\n",
    "* Cross-validation\n",
    "* Regression\n",
    "  * Linear models and neural networks\n",
    "* Classification\n",
    "  * Linear models and neural networks\n",
    "\n",
    "Moreover, working with the popular __pandas__ library is presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e227473dd8d22f4f1225d7205a31ca60",
     "grade": false,
     "grade_id": "cell-aebb53f1296f7f25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e290e410fc45d68147fe4d649471c31",
     "grade": false,
     "grade_id": "cell-47de89ead89817f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import rgb2hex\n",
    "import seaborn as sns\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6d85085225a859e7b9cc5299ee06348",
     "grade": false,
     "grade_id": "cell-e104b64c287bccc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data Set\n",
    "In this exercise we will use a [data set hosted on Kaggle](https://www.kaggle.com/hankelea/system-identification-of-an-electric-motor).\n",
    "For ease of use and simplicity, we are going to work on a reduced version though, that is provided with this notebook already.\n",
    "\n",
    "The data set represents lab measurements from a test bench, where a three-phase electric motor was externally speed-controlled while being induced with different voltages, leading to corresponding currents and, thus, torque on the motor. In terms of reinforcement learning the electric motor can be understood as the environment, the observed electrical state of the motor as the state vector and the applied voltages/discrete switching states of the converter as the actions.\n",
    "\n",
    "### Context\n",
    "The most important aspect of electric vehicles is their efficiency or achievable range. \n",
    "In order to achieve high efficiency, it is essential to avoid the over-dimensioning of the drive train during the design phase.\n",
    "This can only be achieved if the dynamic behavior of the drive train is accurately known by the controller/agent. \n",
    "The task of the controller is to achieve a desired torque at the wheels of the car by controlling the currents of the electric motor.\n",
    "\n",
    "Control, in this context, is about choosing appropriate, discrete switching states for the transistors in the attached power electronic unit, that affect the voltage applied to the motor.\n",
    "Knowing which switching state leads to what current change at each point in time helps the controller to choose an appropriate action.\n",
    "Since in the real world many parasitic, intangible and volatile effects are imposed on currents formation, modeling them statistically is an auspicious endeavor.\n",
    "\n",
    "### Features\n",
    "\n",
    "Most of the time, a data set is a table/matrix/2d-tensor with labeled columns (and rows).\n",
    "\n",
    "Here, rows are consecutively enumerated while columns are the following:\n",
    "\n",
    "* __n_1k__: elementary vector applied between (k-1) and k. The elementary vector denotes the label-encoded switching state in the converter.\n",
    "* __id_k__: d-current at k in A (Ampere).\n",
    "* __iq_k__: q-current at k in A (Ampere). \n",
    "* __epsilon_k__: rotor angle at k in rad (Radian).\n",
    "* __n_k__: elementary vector applied between k and (k+1). \n",
    "* __id_k1__: d-current at (k+1) in A (Ampere).\n",
    "* __iq_k1__: q-current at (k+1) in A (Ampere).\n",
    "\n",
    "[Read up on d/q-transformation](https://en.wikipedia.org/wiki/Direct-quadrature-zero_transformation) to understand the coordinate notation of the currents (assuming the amplitude-invariant transformation than the power-invariant).\n",
    "\n",
    "The enlisted features will be utilized for either regression or classification.\n",
    "Depending on the task, the features denoting input and target quantities will vary, such that the specific mapping will be discussed later.\n",
    "\n",
    "Lab measurements from a motor are commonly time series data.\n",
    "Yet, here the records are shuffled, and we only have the link between two elementary vectors (switching states/ actions) in a row itself.\n",
    "\n",
    "### Optional Literature\n",
    "If you find the following exercise intriguing and you would like to keep on investigating, have a look at the scientific papers that describe the background to the underlying data set:\n",
    "* [Part 1](https://arxiv.org/pdf/2003.07273.pdf)\n",
    "* [Part 2](https://arxiv.org/pdf/2003.06268.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f03315c72b572121989d8e4624cae070",
     "grade": false,
     "grade_id": "cell-b27844f3da8cecb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Load CSV data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cf86d874f5395ff5a1f398c5de3384c",
     "grade": false,
     "grade_id": "cell-422cb7f5e6909a9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The pandas library makes reading csv files easy, next to its superb data wrangling utilities. It is also able to read and save archived tabular data as in this exercises' case.\n",
    "\n",
    "Think of pandas as a layer on top of numpy, that is optimized for tabular data and attaches column/row names to a matrix.\n",
    "\n",
    "The main pandas class is the so-called _DataFrame_ which holds a table, and the _Series_  that denotes a single column.\n",
    "Both classes do not only consist of arrays representing the data itself but also of an index (row labeling), that affects basic arithmetic operations between different instantiated objects.\n",
    "\n",
    "Please refer to the pandas notebooks from exercise 1 for a refresher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6983a745f7900a24269d4b48160c65e",
     "grade": false,
     "grade_id": "cell-37acd3211bc3591f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('reduced_emotor.zip')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd21f70ebc2118e91364d8d20551162a",
     "grade": false,
     "grade_id": "cell-7808b756256de207",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Adding new features\n",
    "There are two main ways to add further features to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fef131cce1e6919493c338a1075567a5",
     "grade": false,
     "grade_id": "cell-fb6e9f4fc33dd9a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new feature \"pairs\"\n",
    "df = df.assign(pairs=lambda r: r.n_1k.astype(str)+'->'+r.n_k.astype(str))\n",
    "# alternatively\n",
    "df['pairs_alternative'] = df.n_1k.astype(str) + '->' + df.n_k.astype(str) \n",
    "\n",
    "assert all(df.pairs == df.pairs_alternative)\n",
    "# remove a feature\n",
    "del df['pairs_alternative']\n",
    "# alternatively\n",
    "# df = df.drop(['pairs_alternative'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "787bd4cc5802b6f6f55972f25db20236",
     "grade": false,
     "grade_id": "cell-aec6adffa5403150",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e13802e23da24eba799ef0a6f43a4874",
     "grade": false,
     "grade_id": "cell-a3eeef12cf6c43c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.pairs.value_counts().sort_index().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e92fb2c550bfac0d3ef919e2036b8887",
     "grade": false,
     "grade_id": "cell-929235a4f8d308fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We recognize, that all transitions within a row appear uniformly throughout the dataset, which is not the case for the original kaggle data but has been intentionally rectified for this exercise's reduced version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8f3d72eb30ee93ae7c22f2ff55cc928",
     "grade": false,
     "grade_id": "cell-2a311570ae5f45b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "Before any feature engineering should be conducted, it is advisable to investigate the general structure of the data.\n",
    "\n",
    "This process is called exploratory data analysis (EDA) and usually comprises \n",
    "* distribution visualizations, \n",
    "* missing data detection, \n",
    "* substitution of such, \n",
    "* linear correlation analysis,\n",
    "* time series analysis\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef738372f632a5d434d7649bf4d21b78",
     "grade": false,
     "grade_id": "cell-b3d8f77b0430d827",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Distribution visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c2351d31a3909996df6b266d014d5af",
     "grade": false,
     "grade_id": "cell-f5ad227060c3bfb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_dist(_df):\n",
    "    fig, axes = plt.subplots(1,2,sharex=True, sharey=True)\n",
    "    for c, ax in zip(['n_1k', 'n_k'], axes.flatten()):\n",
    "        sns.countplot(x=c, data=_df,  ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0abf7b287835eab8dbed691a9a5c334",
     "grade": false,
     "grade_id": "cell-c39f8271e9fcbf72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_dist(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5db8ed12bb50f3ea70d9ab46efca7b5",
     "grade": false,
     "grade_id": "cell-dbd820d77bc013a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Each elementary vector occurs around 350 thousand times for both, at time _k_ and time _k-1_ .\n",
    "\n",
    "In the following we plot the distribution of the currents and the motor angle unfolded over each elementary vector.\n",
    "\n",
    "For this we utilize pandas' _groupby()_ function, which helps to group (and even aggregate) the complete data set with respect to a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d17e0acfab02e8eab2f38889b1a0293e",
     "grade": false,
     "grade_id": "cell-773db26050061bc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzed_cols = [c for c in df if c not in ['n_k', 'n_1k', 'pairs']]\n",
    "\n",
    "# prepare colors\n",
    "color_list = plt.cm.Set3(np.linspace(0, 1, 12))\n",
    "color_d = {'id_k': rgb2hex(color_list[0]),\n",
    "           'id_k1': rgb2hex(color_list[0]),\n",
    "           'iq_k': rgb2hex(color_list[1]),\n",
    "           'iq_k1': rgb2hex(color_list[1]),\n",
    "           'epsilon_k': rgb2hex(color_list[2])}\n",
    "\n",
    "def dist_plot(dframe, cols_to_plot):\n",
    "    unique_elem_vecs = dframe['n_k'].nunique()\n",
    "    # create plot\n",
    "    fig, axes = plt.subplots(nrows=unique_elem_vecs, ncols=len(cols_to_plot), \n",
    "                             sharex='col', sharey='col', figsize=(3*unique_elem_vecs, 10))\n",
    "    for k, _df in dframe.groupby('n_k'):\n",
    "        for i, c in enumerate(cols_to_plot):\n",
    "            sns.histplot(_df[c], ax=axes[k-1, i], color=color_d.get(c, None))\n",
    "            if i == 0:\n",
    "                axes[k-1, i].set_ylabel(f'n_k = {k}')\n",
    "            if k == 1:\n",
    "                axes[k-1, i].set_title(c)\n",
    "            xlbl = c if k == 7 else ''\n",
    "            axes[k-1, i].set_xlabel(xlbl)\n",
    "\n",
    "    plt.tight_layout()\n",
    "dist_plot(df, analyzed_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a1c9d0dc76d3c22d8b029f4f598c87a",
     "grade": false,
     "grade_id": "cell-0246df5fc7d9b075",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_(Double-click picture if it appears too small)_.\n",
    "\n",
    "It seems that _epsilon_k_ exhibits a sine shaped histogram while its range covers $[-\\pi, \\pi]$.\n",
    "\n",
    "More subtle, we recognize a semi-sphere shape of the 2d histogram between the currents (remember, d and q currents are to be plotted perpendicular to each other).\n",
    "It might be auspicious, to add another feature denoting the current vector norm sqrt(id^2 + iq^2).\n",
    "From the principle of d/q-transformation, we know that the $\\ell_2$-norm of the d- and q-current denotes the current amplitude in the system.\n",
    "From that we recognize, that smaller current magnitudes seem more likely than higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "714144e6ac64e5c1339db364fa629a30",
     "grade": false,
     "grade_id": "cell-7368c69fea2082d4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Task 1: Add features and plot their distributions\n",
    "* Add sine and cosine of the rotor angle to the dataframe\n",
    "* Add the current vector norm of both time steps to the dataframe: $i_{norm} = \\sqrt{i_d^2 + i_q^2}$\n",
    "* Plot their distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4575b623e9f2c86de38f731f79cd648d",
     "grade": true,
     "grade_id": "cell-273354ee2c57bdb1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b607121fb58a941bcbf989e8b329f32b",
     "grade": true,
     "grade_id": "cell-dbd6ba2be898d482",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "151da6689539b0e9293c7982c6c40f89",
     "grade": true,
     "grade_id": "cell-f7e3783625157297",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c612f1d5446d167a25843d4bf667663",
     "grade": false,
     "grade_id": "cell-2e6ec9eb45edb784",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Scatter plots\n",
    "Probably, the previous currents are good indicators for the next currents.\n",
    "Let's visualize this relationship unfold over each transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02fd633ba699bf23e6451c0cc7641ac8",
     "grade": false,
     "grade_id": "cell-d3b911cceb0a7f66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_cols = 7\n",
    "n_rows = 7\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, \n",
    "                         sharex=True, sharey=True, figsize=(20, 20))\n",
    "df = df.sort_values(by='pairs')\n",
    "for ax, (k, _df) in zip(axes.flatten(), df.groupby('pairs')):\n",
    "    t_prev, t = k.split('->')\n",
    "    ax.scatter(_df.id_k, _df.id_k1, s=.5, label='d')\n",
    "    ax.scatter(_df.iq_k, _df.iq_k1, s=.5, label='q')\n",
    "    if t == '1':\n",
    "        ax.set_ylabel(f'n_1k = {t_prev}\\n i_k1 in A')\n",
    "    if t_prev == '7':\n",
    "        ax.set_xlabel(f'i_k in A\\n n_k = {t}')\n",
    "    ax.grid(alpha=.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.legend(ncol=2, loc='lower center', bbox_to_anchor=(.5, 1), bbox_transform=fig.transFigure, markerscale=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "173f700090b47469eaf7fb99b33aaa98",
     "grade": false,
     "grade_id": "cell-d2c046101730d429",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Apparently, transitions to switching_state == 1 come with far less scatter than for other switching states.\n",
    "Beyond this, the overall scatter seems relatively linear.\n",
    "\n",
    "We investigate this by a linear correlation heat map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb02f7ea0b52fcad6bd88ab744bf1835",
     "grade": false,
     "grade_id": "cell-845c3e1862504d6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Linear Correlation\n",
    "The [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) indicates the strength of the linear correlation between two random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0192838b37b5ad7be5a83e6a3a5cbaa",
     "grade": false,
     "grade_id": "cell-e6507791679213e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "_ = sns.heatmap(corr, mask=mask, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cd00cf5e257b881f9068d2e8c667f84",
     "grade": false,
     "grade_id": "cell-45437311f7e70024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It becomes obvious, that features from previous time steps exhibit the strongest linear correlation with those of the actual time step.\n",
    "\n",
    "This suggests a strong timely dependency, currents follow a [non-stationary process](https://en.wikipedia.org/wiki/Stationary_process).\n",
    "\n",
    "It is very likely, that more history information for each point in time would reveal precious patterns for an exact forecast of the currents, but these are not available here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f3bdc8e898ed751a5d5dff08c8c5369",
     "grade": false,
     "grade_id": "cell-3d387b56880b066c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "568137a1d9354f6f75950990c5baaff9",
     "grade": false,
     "grade_id": "cell-dde17289924955d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before we start modeling, we need to agree on a certain cross-validation (CV) technique.\n",
    "We will choose a method from scikit-learn.\n",
    "\n",
    "Please install it with\n",
    "```\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "Since distribution of elementary vectors is rather balanced, we will go with a simple CV: 5-Fold CV.\n",
    "\n",
    "Recall from the lecture what K-Fold CV stands for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36b86b8fd13d9b08f1d59f6322f51618",
     "grade": false,
     "grade_id": "cell-d3c24d634e4afe84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![](kfold-cv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85ff6c93125738ab151e49abb1961391",
     "grade": false,
     "grade_id": "cell-337cbc5aaf52aad6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdd1d053acea3ff2902cfaf08392f6e3",
     "grade": false,
     "grade_id": "cell-c067bf5e788d3108",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0173242ca4e5144210debf3d229de08e",
     "grade": false,
     "grade_id": "cell-613a9001732e0616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Regression\n",
    "We start off with a regression example: Given the data set, predict the future currents at any time.\n",
    "\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cf1ad5100e3c6267c8a57c8fafeaedb",
     "grade": false,
     "grade_id": "cell-3022be79207dcf45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "\n",
    "# more feature engineering\n",
    "enriched_df = df.assign(# The following are non-task-specific featurs\n",
    "                        id_k_sqrd=lambda x: x.id_k**2,\n",
    "                        iq_k_sqrd=lambda x: x.iq_k**2,\n",
    "                        iq_x_id_k=lambda x: x.id_k*x.iq_k,\n",
    "                        i_norm_sqrd=lambda x: x.i_norm_k**2,\n",
    "                        id_k_log=lambda x: np.log1p(x.id_k.abs()),\n",
    "                        iq_k_log=lambda x: np.log1p(x.iq_k),\n",
    "                        id_k_exp=lambda x: np.exp(-x.id_k.abs()),\n",
    "                        iq_k_exp=lambda x: np.exp(-x.iq_k.abs()),\n",
    "                        i_norm_log=lambda x: np.log1p(x.i_norm_k),\n",
    "                        i_norm_exp=lambda x: np.exp(x.i_norm_k.abs()),\n",
    "                        id_plus_iq=lambda x: x.id_k+x.iq_k,\n",
    "                        id_minus_iq=lambda x: x.id_k-x.iq_k,\n",
    "                        id_over_iq=lambda x: x.id_k/ (x.iq_k + 1),\n",
    "                        iq_over_id=lambda x: x.iq_k/ (x.id_k - 1),\n",
    "    \n",
    "                        # The following features encompass the whole data set. Is this problematic?\n",
    "                        #id_pairgroup_normed=lambda x: x.id_k - df.groupby('pairs')['id_k'].transform('mean'),\n",
    "                        #iq_pairgroup_normed=lambda x: x.iq_k - df.groupby('pairs')['iq_k'].transform('mean'),\n",
    "                        #id_ngroup_normed=lambda x: x.id_k - df.groupby('n_k')['id_k'].transform('mean'),\n",
    "                        #iq_ngroup_normed=lambda x: x.iq_k - df.groupby('n_k')['iq_k'].transform('mean'),\n",
    "    \n",
    "                        # The following are features taken from https://ieeexplore.ieee.org/document/9545442\n",
    "                        # You can try them for your neural network training. They should significantly boost performance\n",
    "                        id_k_cos=lambda x : x.id_k * x.cos_eps_k,\n",
    "                        iq_k_cos=lambda x : x.iq_k * x.cos_eps_k,\n",
    "                        id_k_sin=lambda x : x.id_k * x.sin_eps_k,\n",
    "                        iq_k_sin=lambda x : x.iq_k * x.sin_eps_k,\n",
    "                        id_k_sqrd_sin=lambda x : x.id_k_sqrd * x.sin_eps_k,\n",
    "                        iq_k_sqrd_sin=lambda x : x.iq_k_sqrd * x.sin_eps_k,\n",
    "                        id_k_sqrd_cos=lambda x : x.id_k_sqrd * x.cos_eps_k,\n",
    "                        iq_k_sqrd_cos=lambda x : x.iq_k_sqrd * x.cos_eps_k,\n",
    "                        iq_x_id_k_cos=lambda x : x.iq_x_id_k * x.cos_eps_k,\n",
    "                        iq_x_id_k_sin=lambda x : x.iq_x_id_k * x.sin_eps_k,\n",
    "                        sin_2eps_k=lambda x : np.sin(2*df.epsilon_k), \n",
    "                        cos_2eps_k=lambda x : np.cos(2*df.epsilon_k), \n",
    "                        id_k_sin_2eps_k=lambda x : x.id_k * x.sin_2eps_k, \n",
    "                        id_k_cos_2eps_k=lambda x : x.id_k * x.cos_2eps_k, \n",
    "                        iq_k_sin_2eps_k=lambda x : x.iq_k * x.sin_2eps_k, \n",
    "                        iq_k_cos_2eps_k=lambda x : x.iq_k * x.cos_2eps_k, \n",
    "                      )\n",
    "\n",
    "y_cols = ['id_k1', 'iq_k1', 'i_norm_k1']\n",
    "x_cols = [c for c in enriched_df if c not in y_cols + ['pairs'] ]\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(enriched_df)):\n",
    "    x_train, y_train = enriched_df.loc[train_index, x_cols], enriched_df.loc[train_index, y_cols]\n",
    "    x_test, y_test = enriched_df.loc[test_index, x_cols], enriched_df.loc[test_index, y_cols].values\n",
    "    \n",
    "    # feature aggregations over the training set\n",
    "    model = LinearRegression()\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    prediction = model.predict(x_test)\n",
    "\n",
    "    result_string = ', '.join(f'{y_cols[j]}: {mse(y_test[:, j], prediction[:, j]):.2f} A²' for j in range(len(y_cols)))\n",
    "    print(f'Fold {i}: \\nMSE:\\t', result_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ae836f2e9b4ad41a26d887efc51a5c0",
     "grade": false,
     "grade_id": "cell-6e815061baed0ec3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The error is homogenous across folds, which affirms our CV strategy.\n",
    "\n",
    "However, the estimation error is fairly large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eda32b5f4c7a0f7ea65681469b71357",
     "grade": false,
     "grade_id": "cell-e2446a51101d2445",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Task 2: What did we miss to do before fitting the linear model?\n",
    "Two important preprocessing steps were missed before conducting model training.\n",
    "Which might they be? It has something to do with categorical values and value ranges.\n",
    "\n",
    "In the following, rectify this blunder, and prepare the data appropriately.\n",
    "* Conduct the CV again.\n",
    "* Plot the estimation error of the last fold over the corresponding ground truth.\n",
    "\n",
    "Can you think of more features? Do you think there might be a problem with those features that were aggregated over the full data set in the beginning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81a440524a645b1b0c0a192f1a3c07a6",
     "grade": true,
     "grade_id": "cell-bca37bee3beb1da5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69a29987b5ca1b6a66308a37dd7234bd",
     "grade": true,
     "grade_id": "cell-d9f9c645f73de47c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4abd966d14dd4bf382bdeef9becc1db1",
     "grade": true,
     "grade_id": "cell-c141066ee4b61c0a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dde7f1da374978abae171be9e5e9de9",
     "grade": true,
     "grade_id": "cell-100b0c82137fbaa7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f1816aa09449824ac9b5cda68151d4d",
     "grade": true,
     "grade_id": "cell-aa8d7b637f1fee4b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d474cb3828cef26b721b8c4abbee5437",
     "grade": false,
     "grade_id": "cell-c095dbc26d1cee80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Neural Networks\n",
    "In the following, a neural network class is given, which can be used the very same way as the scikit-learn linear model.\n",
    "\n",
    "For the imports to work, please install tensorflow 2 and scikeras:\n",
    "\n",
    "```\n",
    "pip install scikeras tensorflow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "421d4b1669d94d25def51f084ee8f26e",
     "grade": false,
     "grade_id": "cell-2d6fc14ffdb1a429",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.optimizers as opts\n",
    "from tensorflow.keras import layers, regularizers, initializers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from scikeras.wrappers import KerasRegressor, KerasClassifier\n",
    "\n",
    "def build_mlp_model(x_shape=(100, 1, 10),  # input shape to the NN\n",
    "                  n_layers=1,  # number of hidden layers\n",
    "                  n_units=64,  # number of neurons per hidden layer\n",
    "                  kernel_reg=1e-9,  # kernel regularization strength\n",
    "                  activity_reg=1e-9,  # activity regularization strength\n",
    "                  bias_reg=1e-9,  # bias regularization strength\n",
    "                  activation='relu',  # activation function\n",
    "                  lr_rate=1e-5,  # learning rate (step size)\n",
    "                  loss='mse',  # cost function for training (here, MSE)\n",
    "                  n_targets=1,  # number of neurons in output layer\n",
    "                  seed=None, print_summary=True, **kwargs):\n",
    "    \"\"\"build mlp model\"\"\"\n",
    "\n",
    "    cfg = {\n",
    "        'units': int(n_units),\n",
    "        'kernel_regularizer': regularizers.l2(kernel_reg),\n",
    "        'activity_regularizer': regularizers.l2(activity_reg),\n",
    "        'bias_regularizer': regularizers.l2(bias_reg),\n",
    "        'kernel_initializer': initializers.lecun_normal(seed=seed),\n",
    "        'activation': activation,\n",
    "    }\n",
    "    \n",
    "    # keras functional API\n",
    "    x = layers.Input(shape=x_shape)  # Input layer\n",
    "    y = layers.Dense(**cfg)(x)  # Hidden layer (feed-forward)\n",
    "    if n_layers > 1:\n",
    "        for i in range(n_layers-1):\n",
    "            y = layers.Dense(**cfg)(y)   # Hidden layer (feed-forward)\n",
    "    y = layers.Dense(n_targets)(y)  # Output layer (feed-forward)\n",
    "\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    model.compile(optimizer=opts.Adam(learning_rate=lr_rate), loss=loss)\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "    \n",
    "class MLP:\n",
    "    \"\"\"Multi-Layer Perceptron with Scikit-learn API\"\"\"\n",
    "    \n",
    "    def __init__(self, wrapper, config):\n",
    "        self.wrapper = wrapper(model=build_mlp_model, **config)\n",
    "        #super().__init__(model=build_mlp_model, **KerasRegressor_config)\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \n",
    "        # Neural Network training encompasses validation against another validation \n",
    "        #  set (next to those of kfold CV), on which overfitting is monitored per epoch\n",
    "        if kwargs.get('validation_data', None) is None:\n",
    "            # determine validation set dynamically\n",
    "            val_ratio = 0.1  # 10% validation data\n",
    "            if isinstance(X, pd.DataFrame): X = X.values\n",
    "            if isinstance(y, pd.DataFrame): y = y.values\n",
    "            n_val_set = int(val_ratio * len(X))\n",
    "            X, X_val = X[:-n_val_set, :], X[-n_val_set:, :]\n",
    "            if len(y.shape) == 1:\n",
    "                y = y[:, np.newaxis]\n",
    "            y, y_val = y[:-n_val_set, :], y[-n_val_set:, :]\n",
    "            kwargs['validation_data'] = (X_val, y_val)\n",
    "        \n",
    "        # Add learn scheduling\n",
    "        #  Early stopping: Stop training if validation error plateaus\n",
    "        #  ReduceLrOnPlateau: If loss on train set plateaus, reduce learn rate\n",
    "        kwargs['callbacks'] = [EarlyStopping(monitor='val_loss',\n",
    "                                              min_delta=1e-3,\n",
    "                                              patience=10,\n",
    "                                              verbose=1),\n",
    "                                ReduceLROnPlateau(monitor='loss',\n",
    "                                                  patience=5,\n",
    "                                                  factor=0.5), ]\n",
    "        kwargs['epochs'] = 20\n",
    "        kwargs['batch_size'] = 64\n",
    "        ret = self.wrapper.fit(X, y, **kwargs)\n",
    "        return ret\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Using the model's current parameters, predict y using x\n",
    "        # Should be used after fitting the model\n",
    "        prediction = self.wrapper.predict(x)\n",
    "        return prediction\n",
    "    \n",
    "    def predict_proba(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "class MLP_Regressor(MLP):\n",
    "    def __init__(self, len_x_cols, len_y_cols, print_summary=True, **kwargs):\n",
    "        # Hard coded build function and kwargs\n",
    "        KerasRegressor_config = {'batch_size': 128,  # how many samples per batch\n",
    "                               'epochs': 99,  # amount of iterations over the data set\n",
    "                               'activation': 'relu',  # activation function\n",
    "                               'n_layers': 1,  # no. hidden layers\n",
    "                               'n_units': 32,  # no. hidden units/neurons\n",
    "                               'kernel_reg': 1e-9,  # kernel l2 regularization strength\n",
    "                               'activity_reg': 1e-9,  # activity l2 regularization strength\n",
    "                               'bias_reg': 1e-9,  # bias l2 regularization strength\n",
    "                               'lr_rate': 5e-4,  # learn rate\n",
    "                               'n_targets': len_y_cols,  # no. targets\n",
    "                               'loss': 'mse',  # loss function\n",
    "                               'verbose': 1,  # verbosity\n",
    "                               'x_shape': (len_x_cols,),  # input shape into NN\n",
    "                               'print_summary': print_summary\n",
    "                               }\n",
    "\n",
    "        super().__init__(wrapper=KerasRegressor, config=KerasRegressor_config)\n",
    "        \n",
    "class MLP_Classifier(MLP):\n",
    "    def __init__(self, len_x_cols, len_y_cols, print_summary=True, **kwargs):\n",
    "        # Hard coded build function and kwargs\n",
    "        KerasClassifier_config = {'batch_size': 128,  # how many samples per batch\n",
    "                               'epochs': 99,  # amount of iterations over the data set\n",
    "                               'activation': 'relu',  # activation function\n",
    "                               'n_layers': 1,  # no. hidden layers\n",
    "                               'n_units': 32,  # no. hidden units/neurons\n",
    "                               'kernel_reg': 1e-9,  # kernel l2 regularization strength\n",
    "                               'activity_reg': 1e-9,  # activity l2 regularization strength\n",
    "                               'bias_reg': 1e-9,  # bias l2 regularization strength\n",
    "                               'lr_rate': 5e-4,  # learn rate\n",
    "                               'n_targets': len_y_cols,  # no. targets\n",
    "                               'verbose': 1,  # verbosity\n",
    "                               'x_shape': (len_x_cols,),  # input shape into NN\n",
    "                               'print_summary': print_summary,\n",
    "                               'target_type': \"multiclass\"\n",
    "                               }\n",
    "\n",
    "        super().__init__(wrapper=KerasClassifier, config=KerasClassifier_config)\n",
    "        \n",
    "        \n",
    "    def predict_proba(self, x):\n",
    "        # Instead of predicting the classes directly, this gives the probabilities for each class \n",
    "        # estimated by the model's current parameters\n",
    "        prediction = self.wrapper.predict_proba(x)\n",
    "        return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6874cae787b809d552828d93a8e9b7b1",
     "grade": false,
     "grade_id": "cell-2279090db77d2faa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Task 3: Conduct CV under the same regression task as above.\n",
    "Is a neural network better?\n",
    "\n",
    "Be aware that training time of a neural network is substantially longer than for linear models. If you face out-of-memory errors, please reduce the n_units parameter, the batch_size parameter, or subsample the data set.\n",
    "\n",
    "__Hint:__ For this task, use the MLP_Regressor class. You can use it in the same way as the linear regressor above.\n",
    "\n",
    "Bonus task: Try to tweak the hyperparameters of the NN in order to achieve a better performance.\n",
    "\n",
    "If you find a decent setting, you are invited to post your solution to [kaggle kernels](https://www.kaggle.com/hankelea/system-identification-of-an-electric-motor/kernels). Don't be shy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9af22d7f1b210ca0637f59ba8149c099",
     "grade": true,
     "grade_id": "cell-84e19a062aa47b0a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1a56c756d22a19fbf018cb30e03f7b4",
     "grade": true,
     "grade_id": "cell-67e0d0c697d6211d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7769d1d07db660ceaf11c5bbe512189e",
     "grade": true,
     "grade_id": "cell-d5d9aba25ae5a099",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3486fd9dc76fe8c90700ed340da44cfc",
     "grade": false,
     "grade_id": "cell-3dbcfd14e582584d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Classification\n",
    "We swap the task from regression to classification by predicting __n_k__ given all other features including currents at time k+1.\n",
    "\n",
    "We start off with [logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) (which is a classifying model despite its name).\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f56b295714699b5c511edcf803160312",
     "grade": false,
     "grade_id": "cell-2a4a1f71cff1c8c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, log_loss  # cross-entropy error\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# instantiate a logistic regression model with the following arguments for faster training\n",
    "LogisticRegression(solver='sag', multi_class='multinomial', max_iter=30, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a90fff943b4d6529838387f98f61482e",
     "grade": false,
     "grade_id": "cell-c0ddeecf30589a68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Please Note__ : We need to restore the original n_k feature but keep n_1k one-hot-encoded in order to use it as input feature. This is only mandatory because of the way LogisticRegression is implemented in scikit-learn. In other frameworks you might have to keep the one-hot-encoded output vector as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f261361b2d5b1bd7c3a83cc1199860f5",
     "grade": false,
     "grade_id": "cell-0a2f52bccac20c2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classify_df = ohe_df.drop([c for c in ohe_df if c.startswith('n_k')], axis=1).assign(n_k=enriched_df.n_k)\n",
    "\n",
    "y_cols = 'n_k'\n",
    "x_cols = [c for c in classify_df if c != y_cols]\n",
    "\n",
    "print('start cv..')\n",
    "for i, (train_index, test_index) in enumerate(kf.split(classify_df)):\n",
    "        train_set = classify_df.loc[train_index, :]\n",
    "        test_set = classify_df.loc[test_index, :]\n",
    "        \n",
    "        # scale (here, only x features, since target is multi-class)\n",
    "        x_scaler = MinMaxScaler()\n",
    "        x_train = train_set.loc[:, x_cols]\n",
    "        x_train.loc[:, x_cols_to_scale] = x_scaler.fit_transform(x_train.loc[:, x_cols_to_scale])\n",
    "        y_train = train_set.loc[:, y_cols]\n",
    "        x_test = test_set.loc[:, x_cols]\n",
    "        x_test.loc[:, x_cols_to_scale] = x_scaler.transform(x_test.loc[:, x_cols_to_scale])\n",
    "        y_test = test_set.loc[:, y_cols].values\n",
    "        \n",
    "        # validate\n",
    "        model = LogisticRegression(solver='sag', multi_class='multinomial', max_iter=30, n_jobs=-1).fit(x_train, y_train)\n",
    "        prediction = model.predict_proba(x_test)\n",
    "        \n",
    "        result_string = f'{y_cols}: {log_loss(y_test, prediction):.2f}'\n",
    "        print(f'Fold {i}: \\nCE:\\t', result_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "254291b3008efe3090a9badc035812ee",
     "grade": false,
     "grade_id": "cell-d68af069e36c618c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Using the predictions of a classifier we are able to construct a confusion matrix. It shows us which classes our classifier predicts better than others and where mix-ups have a higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78f4c13f917763f5f46a386246b4fbd6",
     "grade": false,
     "grade_id": "cell-8c1a77f6d53da677",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(confusion_matrix(y_test, np.argmax(prediction, axis=1)+1),\n",
    "             columns=range(1,8)).assign(Ground_Truth=range(1,8)).set_index('Ground_Truth')\n",
    "cm.columns.name = 'Prediction'\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c43b594fb841208d3e74fb97721f90e",
     "grade": false,
     "grade_id": "cell-abc878d72fe34f42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Neural Networks for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1078ef7ee0a366ddfe887ab9c97bc70e",
     "grade": false,
     "grade_id": "cell-4795efe5f992463f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "#### Task 4: Conduct classification CV using Neural Networks\n",
    "\n",
    "As for the final task we want you to use neural networks again. This time you have to solve the same classification task as depicted for the logistic regression. Conduct the CV and print the confusion matrix. Use the __MLP_Classifier__ class for this.\n",
    "\n",
    "__Hint:__ This time, you have to use the one-hot-encoded output vector __n_k_i__ as the target. Therefore, you have to use the __ohe_df__ instead of the __classifier_df__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d97fa1eb4b81ffd971a149107d2c56e",
     "grade": true,
     "grade_id": "cell-b93c6b4f8a1dd517",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87f326053f4fa85e2dafcbb6b871113a",
     "grade": true,
     "grade_id": "cell-d9cc9069b7f47127",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
