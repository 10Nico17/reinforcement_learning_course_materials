{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9dea5c81cd34f5a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 04): Monte-Carlo Methods\n",
    "\n",
    "In this exercise we make use of the racetrack environment (racetrack_environment.py) to test Monte-Carlo methods.\n",
    "\n",
    "The racetrack environment is based on the OpenAI Gym interface (https://gymnasium.farama.org/) depicted in the picture below.\n",
    "\n",
    "![](RL_GYM_racetrack.png)\n",
    "\n",
    "(Source: Wiki, https://www.vecteezy.com/free-vector/car)\n",
    "\n",
    "The agent can send an action to the system - our racetrack env - using the `env.step(action)` function to drive the car in the environment which is given by the following racetrack: \n",
    "\n",
    "![](Racetrack1.png)\n",
    "\n",
    "Here, the red line represents the start line and the goal is to move the car within the yellow course to the white finish line without hitting the wall. \n",
    "If the car hits the wall, it must be returned to the starting line. \n",
    "The information we get from the step function of the environment are\n",
    "- state consisting of the y- and x-postion (`p_y` and `p_x`) and the velocity in x- and y-direction (`v_y` and `v_x`),\n",
    "- `reward`, which will be -1 per step,\n",
    "- `terminated`-flag which indicates if the environment is terminated (in our case if the car has reached the finish line),\n",
    "- `truncated`-flag which is a termination condition outside of the MDP scope, e.g. timelimit, (in our case hitting a wall before the car has reached the finish line),\n",
    "- info (addioninal information, not used here).\n",
    "\n",
    "Our possible actions are to accelerate the car into x- and/or y-direction (positiv or negativ) or do nothing.\n",
    "\n",
    "Accelerate the car will result in chaning the velocity of the car as follows:\n",
    "![](Beschleunigen.png)\n",
    "\n",
    "Breaking the car will result in chaning the velocity of the car as follows:\n",
    "![](break.png)\n",
    "\n",
    "Our possible action-space is therefore `[-1, 0, 1]` which are availabe as tuple or integer number and encoded as exmplained later on.\n",
    "\n",
    "Actions (accelerations in given directions) are encoded according from integer (`a`) to tuple (`a_y`, `a_x`) using the follwoing equations:\n",
    "\n",
    "- `a_y = a//3-1`\n",
    "- `a_x = a%3-1`\n",
    "\n",
    "This is shown in the following diagram:\n",
    "\n",
    "![](Direction_endcoding.png)\n",
    "\n",
    "Please make yourself more familiar with the used environment (racetrack_environment.py) for more informations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the start, please execute the following cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c8cfa434031df78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from racetrack_environment import RaceTrackEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm.notebook import tqdm\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46112ad628791ed0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Execute the follwoing cell to built a race track using the `RaceTrackEnv` as a test scenario.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab28c0c5fbe2404e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Build the course\n",
    "_course_dim = (8, 10)\n",
    "_inner_wall_dim = (2, 6)\n",
    "\n",
    "def build_uturn_course(course_dim, inner_wall_dim):\n",
    "    \"\"\"\n",
    "    Build a race track for the u-turn street scenario.\n",
    "    Start and finish line are placed in the center top and bottom respectively. The course dimension specifications\n",
    "    do not consider a bounding wall around the track, which is inserted additionally. \n",
    "\n",
    "    \"\"\"\n",
    "    track = []\n",
    "    wall_up_bound = course_dim[0]//2 - inner_wall_dim[0] // 2\n",
    "    wall_bottom_bound = course_dim[0]//2 + inner_wall_dim[0]//2\n",
    "    street_width = course_dim[1]//2 - inner_wall_dim[1]//2\n",
    "    # construct course line by line\n",
    "    for i in range(course_dim[0]):\n",
    "        if i < wall_up_bound:\n",
    "            half_street_len = course_dim[1]//2 - 1\n",
    "            track_row = 'W'*(half_street_len//2+1) + 'W-' + 'o'*(half_street_len-1+half_street_len//2)\n",
    "        elif  wall_up_bound <= i < wall_bottom_bound:\n",
    "            track_row = 'W'*street_width + 'W'*inner_wall_dim[1] + 'o'*street_width\n",
    "        else:\n",
    "            track_row = 'W'*(half_street_len//2+1) + 'W+' + 'o'*(half_street_len-1+half_street_len//2)\n",
    "        track.append(track_row)\n",
    "    # add boundary\n",
    "    track = ['W'*course_dim[1]] + track + ['W'*course_dim[1]]\n",
    "    track = ['W'+s+'W' for s in track]\n",
    "    return track\n",
    "    \n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "for row in course:\n",
    "    print(row)\n",
    "    \n",
    "pos_map =  track.course  # overlay track course\n",
    "plt.imshow(pos_map, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce1387b114d55595",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Monte-Carlo-Based Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3672043edcf93af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a first-visit Monte-Carlo algorithm to evaluate the dummy policy as defined below on the U-turn course. The dummy policy turns the car to the right as soon as it stands in front of a wall. Try to understand how the policy works before you start to code. \n",
    "\n",
    "How can we interprete the state values resulting from the evaluation with first-visit Monte-Carlo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32d1e89b52d7ea2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea22080ba0fc3ad7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Algorithm given below.\n",
    "\n",
    "The simple and deterministic dummy policy will always guarantee the car to reach the finish line. Thus, the state values can be interpreted as the number of timesteps that is necessary to reach the goal from that specific state (i.e. position and velocity) if we are following the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-296f673d66265e7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Select course and initialize dummy policy\n",
    "\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "dummy_slow_pi = np.ones([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY]) * 4 \n",
    "\n",
    "dummy_slow_pi[:track.bounds[0]//2, :, 0 , 0] = 5   # go right\n",
    "dummy_slow_pi[:track.bounds[0]//2, -2:, 0 , :] = 6 # go bottom left\n",
    "dummy_slow_pi[-2:, track.bounds[1]//2:, : , 0] = 0 # go top left\n",
    "\n",
    "pi = dummy_slow_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac5467fab5f148f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize the value function\n",
    "values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY])\n",
    "\n",
    "# initialize an empty dict to count the number of visits\n",
    "n_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 500 # number of evaluated episodes\n",
    "no_steps = 2000 # number of allowed timesteps per episode\n",
    "\n",
    "for e in tqdm(range(no_episodes), position=0, leave=True):\n",
    "    \n",
    "    # initialize variables in which collected data will be stored\n",
    "    states = [] # list of tuples\n",
    "    rewards = [] # list of floats\n",
    "    visited_states = set() # set of tuples\n",
    "    first_visit_list = [] # list of booleans\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    # reset environment and start episode\n",
    "    p, v = track.reset()\n",
    "    for k in range(no_steps):\n",
    "        \n",
    "        # unpack the statee information\n",
    "        s_y, s_x = p[0], p[1]\n",
    "        s_vy, s_vx = v[0], v[1]\n",
    "        state_tuple = s_y, s_x, s_vy, s_vx\n",
    "        \n",
    "        # save the momentary state\n",
    "        states.append(state_tuple) \n",
    "        \n",
    "        # check momentary state for first visit\n",
    "        first_visit_list.append(state_tuple not in visited_states)\n",
    "        visited_states.add(state_tuple)\n",
    "        \n",
    "        # choose and perform action\n",
    "        action = track.action_to_tuple(pi[state_tuple])\n",
    "        (p, v), reward, terminated, truncated, _ = track.step(action)\n",
    "        if truncated:\n",
    "            track.reset()\n",
    "        \n",
    "        # save received reward\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # terminate the environment if the finish line was passed\n",
    "        if terminated: \n",
    "            break \n",
    "             \n",
    "    # learn from the collected data\n",
    "    g = 0  \n",
    "    for s, r, first_visit in zip(states[::-1], rewards[::-1], first_visit_list[::-1]): # count backwards\n",
    "        g = gamma * g + r\n",
    "        \n",
    "        if first_visit:\n",
    "            \n",
    "            # Count visits to this state in n_list\n",
    "            n_dict[s] = n_dict.get(s, 0) + 1\n",
    "\n",
    "            # add new return g to existing value\n",
    "            values[s] += 1/n_dict[s] * (g-values[s])\n",
    "            \n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6fe53fdd68a6c909",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To visualize the result of the evaluation, plot the state values as a function of **position only** (so that you get a two dimensional representation of the state value) and in the form of a tabular represenation and a heatmap. In order to omit dependence of the velocity dimensions, use the minimum of the value function with respect to the velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-74fc6bcd5def8261",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def text_print_pos_map(_pos_map):\n",
    "    for row in _pos_map:\n",
    "        print(' '.join(x_size*['{}']).format(*[str(int(r)).zfill(3) for r in row]))\n",
    "        \n",
    "def plot_pos_map(_pos_map):\n",
    "    plt.imshow(_pos_map, cmap='hot', interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "# calculate minimum value with respect to velocities\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "pos_map = np.zeros((y_size, x_size))\n",
    "\n",
    "for s_x in range(x_size):\n",
    "    for s_y in range(y_size):\n",
    "        pos_map[s_y, s_x] = np.min(values[s_y, s_x, :, :])\n",
    "        \n",
    "text_print_pos_map(pos_map)\n",
    "plot_pos_map(-pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54642e38ce9d8a67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) On-Policy $\\varepsilon$-Greedy Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a81f379107be8dd3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Starting with the previously used turn-right-if-wall dummy policy, write an on-policy Monte-Carlo based first-visit $\\varepsilon$-greedy control algorithm to solve the U-turn course. The policy is now stochastic: it does not contain simple action commands for each state, but probabilities for each possible action. Again, please make sure to understand how the stochastic policy works before coding.\n",
    "\n",
    "\n",
    "Make sure to implement an upper bound for episode length (we suggest a boundary of 200 steps). Why do we need a bound like this? What happens to the state values / state-action values if we increase the bound?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2143fc4c280b5b6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-89a131cffdbb5d52",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Algorithm given below.\n",
    "\n",
    "As we can see, the dummy policy allows for the initial episode to be solved very fast. After that, the dummy policy is forgotten and it takes some time until the agent is able to solve the problem again. \n",
    "\n",
    "The limitation of the episode length forces the agent to learn at least after the allowed number of steps were taken. If one would increase the limit, this would mainly inflate the accumulated return, resulting in larger negative action values for the visited states. As long as we do NOT find the goal, action values will correlate with the time limit. If we find the goal reproducible, the action values will drift towards their true optimal value independently from the time limit.\n",
    "\n",
    "If we do not implement a time limit and allow the episode to terminate only by reaching the goal, the accumulated negative return will explode (we will get very large numbers). As we try to act greedy (take the highest rated and not the lowest rated action), low action values would suggest that the goal is not to be found on the path taken previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b686db0a0a7aed59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dummy policy\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "\n",
    "dummy_slow_stoch_pi = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 9])\n",
    "\n",
    "dummy_slow_stoch_pi[  :,   :, :, :, 4] = 1 # set probability of doing nothing to one for every state\n",
    "\n",
    "# set probability to go right:\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 5] = 1 \n",
    "# set probability to do nothing where we want to go right:\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 4] = 0 \n",
    "\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 6] = 1 # probability to go bottom left\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 4] = 0 \n",
    "\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 0] = 1 # probability to go top left\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 4] = 0 \n",
    "\n",
    "pi = dummy_slow_stoch_pi                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9568aa87f2614759",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize action_values and counting dict\n",
    "action_values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 3, 3])\n",
    "n_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "epsilon = 0.1 # exploration probability\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 5000 # number of evaluated episodes\n",
    "no_steps = 200 # number of evaluated timesteps per episode\n",
    "track_maps_l = []  # placeholder for tracks\n",
    "\n",
    "track = RaceTrackEnv(course)\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "\n",
    "for e in tqdm(range(no_episodes), desc='episode', mininterval=2):\n",
    "      \n",
    "    # initialize variables in which collected data will be stored\n",
    "    action_states = [] # list of tuples\n",
    "    rewards = [] # list of floats\n",
    "    visited_action_states = set() # set of tuples\n",
    "    first_visit_list = [] # list of booleans\n",
    "    \n",
    "    pos_map = np.zeros((y_size, x_size)) # initializes a map that can be plotted\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    p, v = track.reset()\n",
    "    for k in range(no_steps):\n",
    "        s_y, s_x = p[0], p[1]\n",
    "        s_vy, s_vx = v[0], v[1]\n",
    "        \n",
    "        pos_map[s_y, s_x] += 1  # mark the visited position on the map\n",
    "        \n",
    "        # execute action (either by following the policy, or by exploring randomly)\n",
    "        if epsilon < np.random.rand(1):\n",
    "            action = np.argmax(pi[s_y, s_x, s_vy, s_vx])\n",
    "        else:\n",
    "            action = random.choice(range(9))\n",
    "        \n",
    "        # save the action state and check for first visit\n",
    "        a = track.action_to_tuple(action)\n",
    "        action_state = track.state_action((p, v), a)\n",
    "        action_states.append(action_state)\n",
    "        first_visit_list.append(action_state not in visited_action_states)\n",
    "        visited_action_states.add(action_state)\n",
    "        \n",
    "        # perform action\n",
    "        (p, v), reward, terminated, truncated, _ = track.step(a)\n",
    "        if truncated:\n",
    "            track.reset()\n",
    "        \n",
    "        # save received reward\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # terminate the environment if the finish line was passed\n",
    "        if terminated:\n",
    "            break \n",
    "    \n",
    "    # learn from the collected data\n",
    "    g = 0   \n",
    "    for r, a_s, first_visit in zip(rewards[::-1], action_states[::-1], first_visit_list[::-1]): # count backwards\n",
    "        g = gamma * g + r\n",
    "        \n",
    "        if first_visit:\n",
    "            \n",
    "            # Count visits to this state in n_list\n",
    "            n_dict[a_s] = n_dict.get(a_s, 0) +  1\n",
    "\n",
    "            # add new return g to existing value\n",
    "            action_values[a_s] += 1/n_dict[a_s] * (g - action_values[a_s])\n",
    "                        \n",
    "            # calculate the new action probabilities\n",
    "            u_best = np.argmax(action_values[a_s[:4]])\n",
    "            pi[a_s[:4]] = epsilon / 9\n",
    "            pi[a_s[:4]][u_best] = 1 - epsilon + epsilon / 9\n",
    "    \n",
    "    \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # optional value map logging\n",
    "    track_maps_l.append(track.course + (pos_map > 0).astype(np.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef5799678637f070",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# animate visited tracks    \n",
    "fig, ax = plt.subplots()\n",
    "image = plt.imshow(track.course, cmap='hot', interpolation='none')\n",
    "time_text = ax.text(0.05, 0.9, '', transform=ax.transAxes)\n",
    "\n",
    "def get_render_func(_track_maps_l):\n",
    "    def animate(it):\n",
    "        track_map = _track_maps_l[it]\n",
    "        #image.set_array(track.course)\n",
    "        image.set_array(track_map)\n",
    "        time_text.set_text(f\"Iteration {it}\")\n",
    "        return image, time_text\n",
    "    return animate\n",
    "\n",
    "def init():\n",
    "    image.set_array(track.course)\n",
    "    return [image]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, get_render_func(track_maps_l), frames=range(0, len(track_maps_l), 100), \n",
    "                              interval=100, blit=True, init_func=init)\n",
    "ani.save(\"solution_2.gif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0861c8750a2997ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![SegmentLocal](solution_2.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66a45f80f155ca39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the code block directly below to test the resulting deterministic greedy policy (several samples are taken in order to show behavior in all different starting positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba1f0a2326526aeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pos_maps_over_eps_l = []\n",
    "no_episodes = 10\n",
    "for e in range(no_episodes):\n",
    "    \n",
    "    pos_map = np.zeros((y_size, x_size))\n",
    "    p, v = track.reset()\n",
    "    for k in range(200):\n",
    "        s_y, s_x = p[0], p[1]\n",
    "        s_vy, s_vx = v[0], v[1]\n",
    "        \n",
    "        pos_map[s_y, s_x] += 1  # exploration map\n",
    "        \n",
    "        action = np.argmax(pi[s_y, s_x, s_vy, s_vx])\n",
    "        a = track.action_to_tuple(action)\n",
    "        action_state = track.state_action((p, v), a)\n",
    "\n",
    "        (p, v), reward, terminated, truncated, _ = track.step(a)\n",
    "\n",
    "        if truncated:\n",
    "            track.reset()\n",
    "\n",
    "        if terminated:\n",
    "            break \n",
    "    pos_map = (pos_map > 0).astype(np.int16)\n",
    "    pos_map +=  track.course  # overlay track course\n",
    "    pos_maps_over_eps_l.append(pos_map)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, get_render_func(pos_maps_over_eps_l),\n",
    "                              frames=range(0, len(pos_maps_over_eps_l), 1), \n",
    "                              interval=500, blit=True, init_func=init)\n",
    "ani.save(\"solution_2_2.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70e585406cef8528",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![SegmentLocal](solution_2_2.gif \"segment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75a92b1a891b9346",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Extra Challenge: A More Complex Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9eb7640363641603",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The course given below poses a substantially harder challenge for Monte-Carlo based algorithms. Why? If you want to try solving it yourself, be aware that it may take much longer until a successful policy is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7fdf744535830e4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Build the course\n",
    "_course_dim = (8, 10)\n",
    "_inner_wall_dim = (2, 6)\n",
    "\n",
    "def build_rect_course(course_dim, inner_wall_dim):\n",
    "    \"\"\"\n",
    "    Build a race track given specifications for the outer cyclic street and inner wall dimensions.\n",
    "    Start and finish line should be placed in the center top. The course dimension specifications\n",
    "    do not consider a bounding wall around the track, which must be inserted additionally.\n",
    "    \n",
    "    Args:\n",
    "        course_dim: 2-tuple, (y-dim, x-dim): The size of the track without outer walls.\n",
    "        inner_wall_dim: 2-tuple (y-dim, x-dim): The size of the inner wall\n",
    "    \n",
    "    \"\"\"\n",
    "    track = []\n",
    "    wall_up_bound = course_dim[0]//2 - inner_wall_dim[0] // 2\n",
    "    wall_bottom_bound = course_dim[0]//2 + inner_wall_dim[0]//2\n",
    "    street_width = course_dim[1]//2 - inner_wall_dim[1]//2\n",
    "    # construct course line by line\n",
    "    for i in range(course_dim[0]):\n",
    "        if i < wall_up_bound:\n",
    "            half_street_len = course_dim[1]//2 - 1\n",
    "            track_row = 'o'*half_street_len + '+W-' + 'o'*(half_street_len-1)\n",
    "        elif  wall_up_bound <= i < wall_bottom_bound:\n",
    "            track_row = 'o'*street_width + 'W'*inner_wall_dim[1] + 'o'*street_width\n",
    "        else:\n",
    "            track_row = 'o'*course_dim[1]\n",
    "        track.append(track_row)\n",
    "    # add boundary\n",
    "    track = ['W'*course_dim[1]] + track + ['W'*course_dim[1]]\n",
    "    track = ['W'+s+'W' for s in track]\n",
    "    return track\n",
    "    \n",
    "course = build_rect_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "for row in course:\n",
    "    print(row)\n",
    "    \n",
    "pos_map =  track.course  # overlay track course\n",
    "plot_pos_map(pos_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6382c23e5d25c036",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-15500169957f16d3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Taking four turns to reach the goal is way harder than taking just two turns. Additionally, the state space is a lot larger now, which leads to much more exploration being necessary until all the states are properly evaluated. Although the course is more complicated, the problem description (\"reach the goal\") and the evironment physics (acceleration, momentum and collision) are still the same. Thus, there is no fundamental reason why Monte-Carlo should not be successful here, we just have to be aware that it will take some time.\n",
    "\n",
    "Fortunately, there are still upcoming lectures where more efficient learning algorithms could be discussed ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9f170e15782def02",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The following screenshot was taken after trying to solve this problem with the same algorithm as presented in task 2). As can be seen, the agent is actually able to solve the racetrack and reach the finish line. But it took about six hours on a very powerful computer to do so.\n",
    "\n",
    "![](FullCourse_MonteCarlo_Solved.png)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
