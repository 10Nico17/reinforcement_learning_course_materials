{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57facccd",
   "metadata": {},
   "source": [
    "# Exercice 13) Deep Deterministic Policy Gradients and Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe402026",
   "metadata": {},
   "source": [
    "In this exercise we will investigate two state-of-the-art algorithms: deep deterministic policy gradient (DDPG) and proximal policy optimization (PPO).\n",
    "\n",
    "We will examine their performance on [Goddard's rocket problem](https://github.com/osannolik/gym-goddard).\n",
    "This environment comes prepackaged in this notebook's folder, so it can be just imported.\n",
    "\n",
    "```\n",
    "First formulated by R. H. Goddard around 1910, this is a classical problem within dynamic optimization and optimal control. The task is simply to find the optimal thrust profile for a vertically ascending rocket in order for it to reach the maximum possible altitude, given that its mass decreases as the fuel is spent and that it is subject to varying drag and gravity.\n",
    "```\n",
    "\n",
    "The gym's observation space is the rocket's vertical position, velocity and mass.\n",
    "\n",
    "The rocket engine is assumed to be throttled such that the thrust can be continuously controlled between 0 to some maximum limit, which translates to an action space $\\mathcal A \\in [0, 1]$.\n",
    "\n",
    "![](rocket_spacex.jpg)\n",
    "(Photo by <a href=\"https://unsplash.com/@spacex?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">SpaceX</a> on <a href=\"https://unsplash.com/s/photos/rocket?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>)\n",
    "  \n",
    "\n",
    "In order for the full notebook to run through, you will also need the stable-baslines3 package.\n",
    "\n",
    "```\n",
    "pip install stable-baselines3[extra]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ccabd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rocket_env import GoddardEnv\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from stable_baselines3 import PPO, DDPG, A2C\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from copy import deepcopy\n",
    "\n",
    "OPTIMAL_CONTROL = 0.0122079818367078\n",
    "RANDOM_AVG = 0.00995\n",
    "RANDOM_BEST = 0.01148"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3a5c7",
   "metadata": {},
   "source": [
    "## 1) DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb652100",
   "metadata": {},
   "source": [
    "Deep Deterministic Policy Gradient (DDPG) was first introduced [2015](https://arxiv.org/abs/1509.02971).\n",
    "It uses four neural networks.\n",
    "The first two we already know from the last exercise introduced Actor-Critic method: a critic to estimate the q-value function and an actor which deterministicly gives us an action for a specific state.\n",
    "Additionally DDPG provides two target networks which are (in the beginning) copies of tthe actor and critic. These target networks are updated time-delayed in a low-pass filter manner.\n",
    "This enhances stability during the learing process.\n",
    "For more information see for example [here](https://spinningup.openai.com/en/latest/algorithms/ddpg.html).\n",
    "\n",
    "During the learning process the critic is updated based on minimizing (!) the follwoing loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "L(w, {\\mathcal D}) = \\underset{(x,u,r,x',d) \\sim {\\mathcal D}}{{\\mathrm E}}\\left[\n",
    "    \\Bigg( \\hat{q}(x,u, w) - \\left(r + \\gamma (1 - d)  \\hat{q}_{}(x',\\pi(x', \\theta_\\text{target}), w_\\text{target}) \\right) \\Bigg)^2\n",
    "    \\right].\n",
    "\\end{equation}   \n",
    "\n",
    ", here the target is calculated using the target networks. $w$ and $w_{target}$ are the parameters of the critic and critic-target networks $\\hat{q}$, respectively and $\\theta_{target}$ define the parameters of the actor-target network.\n",
    "\n",
    "The policy/actor network is updated based on the idea to maximize (!) the expected return\n",
    "\n",
    "\\begin{align}\n",
    "    \\max_{\\theta} \\underset{s \\sim {\\mathcal D}}{{\\mathrm E}}\\left[\\hat{q}(x, \\pi_{}(x, \\theta), w) \\right]\n",
    "\\end{align}  \n",
    "\n",
    ", here $\\theta$ are the parameters of the policy network $\\pi$.\n",
    "\n",
    "The updates are performed off-policy by sampling from an experince replay buffer ${\\mathcal D}$.\n",
    "Since we are dealing with an deterministic actor, exploration during training is achieved by adding noise to the sampled actions.\n",
    "\n",
    "### Task: Implement the DDPG with Pytorch\n",
    "Write an DDPG algorithm using the algXXX in lecture slides 13!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d07d4",
   "metadata": {},
   "source": [
    "Execute the following cell to make use of the defined multi layer perceptron and the plot function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e99e71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    \"\"\"\n",
    "    Defines a multi layer perceptron using pytorch layers and activation funtions\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def plot_reward_trends(logs):\n",
    "    repeats = len(logs['envs'])\n",
    "    ncols = 2\n",
    "    nrows = np.ceil(repeats / ncols, dtype=int)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*8, nrows*4), sharex=True, sharey=True)\n",
    "    for ax, _env, _model in zip(axes.flatten(), logs['envs'], logs['models']):\n",
    "        ax.plot(_env.get_episode_rewards(), label='rewards', color='green')\n",
    "\n",
    "        ax.set_ylabel('Reward')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylim(0, 0.0123)\n",
    "        ax.axhline(OPTIMAL_CONTROL, ls='--', color='red', label='optimal')\n",
    "        ax.axhline(RANDOM_BEST, ls='--', color='orange', label='best rnd actions')\n",
    "        ax.axhline(RANDOM_AVG, ls='--', color='yellow', label='average rnd actions')\n",
    "        ax.legend(loc='lower left')\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(_env.get_episode_lengths(), label='ep lengths')\n",
    "        ax2.legend()\n",
    "        ax2.set_ylabel('Steps')\n",
    "    fig.tight_layout()\n",
    "\n",
    "def test_agent(agent, env):\n",
    "    \"\"\"Deterministic test of the agent in the given env\"\"\"    \n",
    "    num_test_epsidoes = 400\n",
    "    episode_reward = 0\n",
    "    episode_len = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    for j in tqdm(range(num_test_epsidoes)):\n",
    "\n",
    "        episode_len += 1\n",
    "        #action = agent.decide(torch.as_tensor(state, dtype=torch.float32))\n",
    "        action = agent.decide(state, deterministic=True)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        env.render()\n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            break\n",
    "\n",
    "    print(f'Reward during test: {episode_reward}')\n",
    "    print('Optimal control: 0.0122')\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad278f",
   "metadata": {},
   "source": [
    "Execute the following cells to make use of the predefine actor & critic and the replay buffer!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90348b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation=nn.ReLU, act_limit=1):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Sigmoid)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.act_limit * self.pi(state)\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.act_limit * self.pi(state).numpy()\n",
    "    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # torch.cat concatenates action to state\n",
    "\n",
    "        q = self.q(torch.cat([state, action], dim=-1))\n",
    "        return torch.squeeze(q, -1)  # To ensure q has right shape.\n",
    "    \n",
    "class ReplayBuffer:\n",
    "\n",
    "        def __init__(self, obs_dim, action_dim, buffer_size):\n",
    "            \n",
    "            self.state_buf = np.zeros((buffer_size, obs_dim), dtype=np.float32)\n",
    "            self.next_state_buf = np.zeros((buffer_size, obs_dim), dtype=np.float32)\n",
    "            self.action_buf = np.zeros((buffer_size, action_dim), dtype=np.float32)\n",
    "            self.reward_buf = np.zeros(buffer_size, dtype=np.float32)\n",
    "            self.done_buf = np.zeros(buffer_size, dtype=np.float32)\n",
    "            self.ptr, self.size, self.max_size = 0, 0, buffer_size\n",
    "\n",
    "        def push(self, state, action, reward, next_state, done):\n",
    "            \n",
    "            self.state_buf[self.ptr] = state\n",
    "            self.next_state_buf[self.ptr] = next_state\n",
    "            self.action_buf[self.ptr] = action\n",
    "            self.reward_buf[self.ptr] = reward\n",
    "            self.done_buf[self.ptr] = done\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "            self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "        def fetch(self, batch_size=32):\n",
    "            \n",
    "            idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "            \n",
    "            batch = dict(state=self.state_buf[idxs],\n",
    "                         next_state=self.next_state_buf[idxs],\n",
    "                         action=self.action_buf[idxs],\n",
    "                         reward=self.reward_buf[idxs],\n",
    "                         done=self.done_buf[idxs])\n",
    "            \n",
    "            return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfe16e",
   "metadata": {},
   "source": [
    "Here, fill in the following code template to write a DDPG agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1edca219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_agent:\n",
    "    \"\"\"Reference:\n",
    "    https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, env, actor_hidden_size, actor_number_layers, critic_hidden_size, critic_number_layers,\n",
    "                 buffer_size, actor_lr, critic_lr, gamma, batch_size, learning_starts, tau):#, action_noise):\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_starts = learning_starts\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.shape[0]\n",
    "        \n",
    "\n",
    "        # define A&C and the replaybuffer        \n",
    "        self.actor = None\n",
    "        self.critic = None\n",
    "        self.replay_buffer = None\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        self.actor = Actor(obs_dim, act_dim, [actor_hidden_size] * actor_number_layers, nn.ReLU)\n",
    "        self.critic = Critic(obs_dim, act_dim, [critic_hidden_size] * critic_number_layers, nn.ReLU)\n",
    "        self.replay_buffer = ReplayBuffer(obs_dim, act_dim, buffer_size)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # defines target networks\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        \n",
    "        # Uses Adam optimizer (see to ex12 for more explanation)\n",
    "        self.actor_optimizer = Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        # define action noise space for clipping\n",
    "        self.action_space_high = env.action_space.high\n",
    "        self.action_space_low = env.action_space.low\n",
    "\n",
    "    def q_loss(self, data):\n",
    "        ### BEGIN SOLUTION\n",
    "        \n",
    "        state, action, rerwad, next_state, done = data['state'], data['action'], data['reward'], data['next_state'], data['done']\n",
    "            \n",
    "        # Gradient descent for Q-function\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        q = self.critic.forward(state, action)\n",
    "        with torch.no_grad():\n",
    "            q_pi_targ = self.critic_target.forward(next_state, self.actor_target.forward(next_state))\n",
    "            backup = rerwad + self.gamma * (1 - done) * q_pi_targ\n",
    "            \n",
    "        # MSE loss against Bellman backup\n",
    "        return ((q - backup) ** 2).mean()\n",
    "    \n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def pi_loss(self, data):\n",
    "        \"\"\"\n",
    "        Calulate the loss for a gradient ascent(!) step\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        \n",
    "        state = data['state']\n",
    "            \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        return -self.critic.forward(state, self.actor.forward(state)).mean()\n",
    "\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    \n",
    "    def deliberate(self, number_updates_per_step):\n",
    "        \"\"\"\n",
    "        Fetches number_updates_per_step-times from replay_buffer, computes targets and calculates losses to \n",
    "        update the q-function using gradient descent and the policy function using gradient ascent\n",
    "        In the end updates the target networks\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(number_updates_per_step):\n",
    "            ### BEGIN SOLUTION\n",
    "        \n",
    "            data = self.replay_buffer.fetch(self.batch_size)\n",
    "            \n",
    "            # Gradient descent for Q-function\n",
    "            loss_q = self.q_loss(data)\n",
    "            loss_q.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Freeze Q-network so you don't waste computational effort\n",
    "            # computing gradients for it during the policy learning step.\n",
    "            for p in self.critic.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # Gradient ascent step for pi.\n",
    "            loss_pi = self.pi_loss(data)\n",
    "            loss_pi.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "            for p in self.critic.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            # Finally, update target networks by polyak averaging.\n",
    "            with torch.no_grad():\n",
    "                # update Critc-target\n",
    "                for p, p_targ in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    # uses in-place operations \"mul_\", \"add_\" to update target\n",
    "                    # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "                    p_targ.data.mul_(1 - self.tau)\n",
    "                    p_targ.data.add_((self.tau) * p.data)\n",
    "\n",
    "                # update Actor-target\n",
    "                for p, p_targ in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    # uses in-place operations \"mul_\", \"add_\" to update target\n",
    "                    # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "                    p_targ.data.mul_(1 - self.tau)\n",
    "                    p_targ.data.add_((self.tau) * p.data)\n",
    "                           \n",
    "            ### END SOLUTION\n",
    "            \n",
    "    def decide(self, state, deterministic=False, noise_scale=0.1):\n",
    "        \"\"\"\n",
    "        Returns action as nd-array depending on the state, adds scaled noise \n",
    "        and clipps the action depending on the actionspace\n",
    "        \"\"\"\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        action = self.actor.act(torch.as_tensor(state, dtype=torch.float32))\n",
    "        \n",
    "        if deterministic:\n",
    "            return np.clip(action, self.action_space_low, self.action_space_high)\n",
    "        else:\n",
    "            return np.clip(action + 0.1 * np.random.randn(1), self.action_space_low, self.action_space_high)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaacefb",
   "metadata": {},
   "source": [
    "Run the following cell to train your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bde17eef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilhelmk/tools/anaconda3/envs/torch/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13d96db1c144863be5764d67268394e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef55e98ec27453db57f3a4f53a7208d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2a8ab0a9ca43c2bf453101e0eb4d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417d51645f674ba190352a6c168362d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_logs_ddpg = {'envs': [], 'models': []}\n",
    "repeats = 4\n",
    "\n",
    "total_timesteps = 2000\n",
    "\n",
    "for rep in range(repeats):\n",
    "\n",
    "\n",
    "    env = GoddardEnv()\n",
    "\n",
    "    myDDPG_agent = DDPG_agent(env=env, actor_hidden_size=8, actor_number_layers=1,\n",
    "                           critic_hidden_size=8, critic_number_layers=1, buffer_size=int(1e6),\n",
    "                           actor_lr=1e-4, critic_lr=1e-4, gamma=0.999, batch_size=256, learning_starts=100, tau=0.005)\n",
    "\n",
    "\n",
    "    number_updates_per_step = 0\n",
    "    episode_reward = 0\n",
    "    episode_len = 0\n",
    "    rewards = []\n",
    "    episode_len_vec = []\n",
    "\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    for j in tqdm(range(total_timesteps)):\n",
    "\n",
    "        episode_len += 1\n",
    "        number_updates_per_step += 1\n",
    "\n",
    "\n",
    "        action = myDDPG_agent.decide(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        myDDPG_agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        #env.render()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "\n",
    "        if done:\n",
    "            myDDPG_agent.deliberate(number_updates_per_step)\n",
    "            state = env.reset()\n",
    "            rewards.append(episode_reward)\n",
    "            episode_len_vec.append(episode_len)\n",
    "            episode_reward = 0\n",
    "            episode_len = 0\n",
    "            number_updates_per_step = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_logs_ddpg['envs'] += [env]\n",
    "    train_logs_ddpg['models'] += [myDDPG_agent]\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9f142c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No loop matching the specified signature and casting was found for ufunc ceil",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5ee01aea7c39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_reward_trends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_logs_ddpg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-5fc2bec0b8f9>\u001b[0m in \u001b[0;36mplot_reward_trends\u001b[0;34m(logs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mrepeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'envs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mncols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeats\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_model\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'envs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: No loop matching the specified signature and casting was found for ufunc ceil"
     ]
    }
   ],
   "source": [
    "plot_reward_trends(train_logs_ddpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec9414",
   "metadata": {},
   "source": [
    "Use the follwoing cell to test your agent on the env using deterministic actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf718c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b837ef5ee5784d2c849774f2acc43be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward during test: 0.011863225739212613\n",
      "Optimal control: 0.0122\n"
     ]
    }
   ],
   "source": [
    "env = GoddardEnv()\n",
    "test_agent(myDDPG_agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa062069",
   "metadata": {},
   "source": [
    "### Demo: StableBaselines3 usage\n",
    "\n",
    "Alternatively, in the real world, you would use readily available Python packages such as [stable-baselines3](https://github.com/DLR-RM/stable-baselines3) for employment of state-of-the-art algorithms.\n",
    "In what follows below, the DDPG algorithm as utilized by stable-baselines3 is showcased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b1eaafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f32b3785abe4ae08f690fcde1b1676f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2396bd50377d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     model = DDPG(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, batch_size=256, device='cpu',\n\u001b[1;32m     35\u001b[0m              learning_rate=1e-4, gamma=0.999)\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'envs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/anaconda3/envs/torch/lib/python3.7/site-packages/stable_baselines3/ddpg/ddpg.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         )\n",
      "\u001b[0;32m~/tools/anaconda3/envs/torch/lib/python3.7/site-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         )\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/anaconda3/envs/torch/lib/python3.7/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;31m# do as many gradients steps as steps performed during the rollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mgradient_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_steps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/anaconda3/envs/torch/lib/python3.7/site-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# Delayed policy updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/anaconda3/envs/torch/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/anaconda3/envs/torch/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/anaconda3/envs/torch/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_timesteps = int(20e3)\n",
    "\n",
    "\n",
    "class Pbar(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pbar = tqdm(desc='Training', total=total_timesteps)\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"This event is triggered before updating the policy.\"\"\"\n",
    "        # self.pbar.update(self.)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.pbar.update()\n",
    "        return True\n",
    "    \n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[8], vf=[8])])\n",
    "\n",
    "repeats = 5\n",
    "logs = {'envs': [], 'models': []}\n",
    "for _ in range(repeats):\n",
    "    \n",
    "    env = Monitor(GoddardEnv())\n",
    "    \n",
    "    #n_actions = env.action_space.shape[-1]\n",
    "    #noise_var = 4\n",
    "    #noise_theta = 25  # stiffness of OU\n",
    "    #action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), theta=noise_theta * np.ones(n_actions),\n",
    "    #                                        sigma=noise_var * np.ones(n_actions))\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "    \n",
    "    policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=dict(pi=[8] * 1, qf=[8] * 1))\n",
    "    model = DDPG(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, batch_size=256, device='cpu',\n",
    "             learning_rate=1e-4, gamma=0.999)\n",
    "    model.learn(total_timesteps=total_timesteps, callback=Pbar())\n",
    "    logs['envs'] += [env]\n",
    "    logs['models'] += [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_trends(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a1387f",
   "metadata": {},
   "source": [
    "Use the following cell to execute the last trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1\n",
    "#model = DDPG(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, batch_size=64, device='cpu', learning_starts=1280,\n",
    " #            learning_rate=1e-3)\n",
    "max_steps_per_episode = 500\n",
    "tst_logs = {'rewards': []}\n",
    "for ep in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = np.zeros(1, dtype=np.bool)\n",
    "    cum_rew = 0\n",
    "    k = 0\n",
    "    while not np.all(done) and k < max_steps_per_episode:\n",
    "        action, _ = model.predict(state, deterministic=True) # env.action_space.sample()#\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        cum_rew += reward\n",
    "        k += 1\n",
    "\n",
    "    tst_logs['rewards'].append(cum_rew)\n",
    "rewards = np.array(tst_logs['rewards']).ravel()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50f7af",
   "metadata": {},
   "source": [
    "## 2) PPO\n",
    "\n",
    "The [original paper from 2017](https://arxiv.org/abs/1707.06347) for the PPO came up with an idea to combine A2C (having multiple workers) and TRPO (using a trust region to improve the actor).\n",
    "The PPO algorithm achieves this by hard clipping gradients in order to ensure that new policies won't be too far away from old ones.\n",
    "\n",
    "In contrast to DDPG, PPO is an on-policy algorithm. In order to still apply mini-batch training, there is a so-called roll-out-buffer that is filled up with the current policy, on whose base a gradient ascent update would be done.\n",
    "\n",
    "There are [two variants on the PPO algorithm](https://spinningup.openai.com/en/latest/algorithms/ppo.html), from which we will implement the clip variant.\n",
    "\n",
    "The actor (policy) update is computed according to\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_{k+1} &= \\arg \\max_{\\theta} \\underset{\\langle x,u \\rangle \\sim \\pi_{\\theta_k}}{{\\mathbb E}}\\left[\\mathcal L(X,U,\\theta_k, \\theta)\\right], \\\\\n",
    "\\mathcal L(x,u,\\theta_k,\\theta) &= \\min\\left(\n",
    "\\frac{\\pi_{\\theta}(u|x)}{\\pi_{\\theta_k}(u|x)}  A^{\\pi_{\\theta_k}}(x,u), \\;\\;\n",
    "\\text{clip}\\left(\\frac{\\pi_{\\theta}(u|x)}{\\pi_{\\theta_k}(u|x)}, 1 - \\epsilon, 1+\\epsilon \\right) A^{\\pi_{\\theta_k}}(x, u)\n",
    "\\right),\n",
    "\\end{align}\n",
    "where the expectation operator denotes the empirical average across the roll-out-buffer, $\\theta$ the possible next weights of the actor, $\\epsilon$ denotes the threshold within the next update is allowed, and $\\mathcal L$ the long-term return.\n",
    "\n",
    "On the opposite side, the critic's weights $\\omega_k$ are updated through the plain stochastic mini-batch gradient descent on the mean squared error $\\mathcal C$ between immediately seen rewards $r_{k+1}$ and corresponding estimated value $v_{\\phi_k}(x_k)$ across the roll-out-buffer:\n",
    "\\begin{align}\n",
    "\\phi_{k+1} &\\leftarrow \\omega_k - \\eta \\nabla\\mathcal C(X,R_{k+1},\\omega_k), \\\\\n",
    "\\mathcal C(x,r_{k+1}, \\omega_k) &= \\left(v_{\\omega_k} (x_k)- r_{k+1}\\right)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402a64a",
   "metadata": {},
   "source": [
    "### Task: Implement the PPO clip variant with Pytorch\n",
    "Fill in the below code to run the PPO-clip variant on the rocket environment.\n",
    "\n",
    "For simplicity, do not contemplate a vectorized environment for synchronous training (like in A2C or A3C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1722f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochActor(nn.Module):\n",
    "    \"\"\"This stochastic actor learns a Gaussian distribution. While the mean value \n",
    "    is learnt by a full-fledged MLP, the standard deviation is denoted by a\n",
    "    single trainable weight. With Pytorch's distribution package, probabilities \n",
    "    given a certain distribution and an action can be calculated.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # The standard deviation is just one trainable weight\n",
    "        self.log_std = torch.nn.Parameter(\n",
    "                        torch.as_tensor(\n",
    "                            -.5 * np.ones(action_dim, dtype=np.float32)))\n",
    "        # the mean value is estimated by a full MLP\n",
    "        self.mu_net = mlp(list(state_dim) + [8] + list(action_dim), nn.Tanh, nn.Sigmoid)\n",
    "        \n",
    "    def _distribution(self, state):\n",
    "        return torch.distributions.normal.Normal(\n",
    "                self.mu_net(state), torch.exp(self.log_std))\n",
    "    \n",
    "    def forward(self, state, action=None):\n",
    "        pi = self._distribution(state)\n",
    "        # if action is None, logp_a will be, too\n",
    "        if action is None:\n",
    "            logp_a = None\n",
    "        else:\n",
    "            logp_a = pi.log_prob(action).sum(axis=-1)\n",
    "        return pi, logp_a\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"Reference:\n",
    "    https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ppo/ppo.py\n",
    "    \"\"\"\n",
    "    \n",
    "    class RolloutBuffer:\n",
    "        def __init__(self, size, action_dim, state_dim):\n",
    "            self.action_buf = np.zeros((size, action_dim[0]), dtype=np.float32)\n",
    "            self.state_buf = np.zeros((size, state_dim[0]), dtype=np.float32)\n",
    "            self.rew_buf = np.zeros(size+1, dtype=np.float32)\n",
    "            self.val_buf = np.zeros(size+1, dtype=np.float32)\n",
    "            self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "            self.i = 0\n",
    "            self.max_size = size\n",
    "            \n",
    "        def push(self, state, action, reward, value, logp):\n",
    "            \"\"\"Append a sample to the buffer\"\"\"\n",
    "            ### BEGIN SOLUTION\n",
    "            self.state_buf[self.i] = state\n",
    "            self.action_buf[self.i] = action\n",
    "            self.rew_buf[self.i] = reward\n",
    "            self.val_buf[self.i] = value\n",
    "            self.logp_buf[self.i] = logp\n",
    "            self.i += 1\n",
    "            ### END SOLUTION\n",
    "            \n",
    "        def fetch(self, last_val=None, gamma=0.999):\n",
    "            \"\"\"Get all data from the rollout buffer.\n",
    "            Returns a dictionary for state, action, rewards-to-go, advantages, and logp values.\n",
    "            Entries might be of different length across multiple calls to this function as episodes\n",
    "            potentially have different lengths. Rewards-to-go and advantages need to be computed from \n",
    "            rewards, values, and gamma\"\"\"\n",
    "            ### BEGIN SOLUTION\n",
    "            if last_val is not None:\n",
    "                self.rew_buf[self.i] = last_val\n",
    "                self.val_buf[self.i] = last_val\n",
    "            rewards = self.rew_buf[:self.i+1]\n",
    "            values = self.val_buf[:self.i+1]\n",
    "            \n",
    "            # Advantage calculaton            \n",
    "            advantages = rewards[:-1] + gamma * values[1:] - values[:-1]\n",
    "            weights = gamma ** np.arange(advantages.size)\n",
    "            advantages = np.cumsum(weights * advantages[::-1])[::-1]\n",
    "            \n",
    "            # reward\n",
    "            rewards_to_go = np.cumsum(weights * (gamma*rewards[:-1][::-1]))[::-1]\n",
    "            \n",
    "            # normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / np.std(advantages)\n",
    "            \n",
    "            data =  {'state': self.state_buf[:self.i],\n",
    "                     'action': self.action_buf[:self.i],\n",
    "                     'rewards_to_go': rewards_to_go.copy(),  # make stride positive\n",
    "                     'advantages': advantages, \n",
    "                     'logp': self.logp_buf[:self.i]}\n",
    "            self.i = 0\n",
    "            \n",
    "            return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in data.items()}\n",
    "            ### END SOLUTION\n",
    "    \n",
    "    def __init__(self, buffer_size, env):\n",
    "        self.buf = self.RolloutBuffer(buffer_size, action_dim=env.action_space.shape,\n",
    "                                 state_dim=env.observation_space.shape)\n",
    "        # use the below attributes for the actor-critic agent\n",
    "        self.actor = None\n",
    "        self.critic = None\n",
    "        self.n_epochs = 8\n",
    "        self.clip_ratio = 0.1\n",
    "        ### BEGIN SOLUTION\n",
    "        \n",
    "        self.actor = StochActor(action_dim=env.action_space.shape, state_dim=env.observation_space.shape)\n",
    "        self.critic = mlp(list(env.observation_space.shape) + [8, 1], nn.Tanh, nn.Tanh)\n",
    "        self.pi_opt = Adam(self.actor.parameters(), lr=1e-3)\n",
    "        self.v_opt = Adam(self.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "        ### END SOLUTION\n",
    "\n",
    "    \n",
    "    def v_loss(self, data):\n",
    "        ### BEGIN SOLUTION\n",
    "        state, rewards_to_go = data['state'], data['rewards_to_go']\n",
    "        return ((self.critic(state).squeeze(-1) - rewards_to_go)**2).mean()\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def pi_loss(self, data):\n",
    "        ### BEGIN SOLUTION\n",
    "        state, action, advantage, logp_old = [data[k] for k \n",
    "                                              in ('state', 'action', 'advantages', 'logp')]\n",
    "        \n",
    "        pi, logp = self.actor(state, action)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clip_adv = torch.clamp(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * advantage\n",
    "        loss_pi = -(torch.min(ratio * advantage, clip_adv)).mean()\n",
    "        \n",
    "        return loss_pi\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def deliberate(self, last_value=None):\n",
    "        \"\"\"Fetch the rollout buffer for all samples.\n",
    "        Train the actor and critic for n_epochs by computing the pi and v\n",
    "        loss iteratively.\n",
    "        The rewards and values buffer needs a zero appended for terminal states,\n",
    "        or the critic's estimate appended in case of intermediate states \n",
    "        that result from timeout. This is denoted by last_value.\"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        batch = self.buf.fetch(last_value)\n",
    "        loss_pi_old = self.pi_loss(batch).item()\n",
    "        loss_v_old = self.v_loss(batch).item()\n",
    "        \n",
    "        # update policy n_epoch times\n",
    "        for _ in range(self.n_epochs):\n",
    "            self.pi_opt.zero_grad()\n",
    "            loss_pi = self.pi_loss(batch)\n",
    "            loss_pi.backward()\n",
    "            self.pi_opt.step()\n",
    "        \n",
    "            self.v_opt.zero_grad()\n",
    "            loss_v = self.v_loss(batch)\n",
    "            loss_v.backward()\n",
    "            self.v_opt.step()\n",
    "        \n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def decide(self, state, deterministic=False):\n",
    "        \"\"\"Receive a state as torch tensor and return a tuple of numpy ndarrays\n",
    "        in the form (action, value, log-probability-of-action-under-pi)\"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        state = torch.as_tensor(state, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            pi = self.actor._distribution(state)\n",
    "            if deterministic:\n",
    "                return pi.mean\n",
    "            else:\n",
    "                a = pi.sample()\n",
    "            logp_a = pi.log_prob(a).sum(axis=-1)\n",
    "            v = self.critic(state)\n",
    "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf33ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "\n",
    "n_episodes = 250\n",
    "max_steps_per_episode = 500\n",
    "train_logs = {'rewards': [], 'envs': [], 'models': []}\n",
    "repeats = 4\n",
    "for rep in range(repeats):\n",
    "    env = Monitor(GoddardEnv())\n",
    "    agent = PPOAgent(max_steps_per_episode, env)\n",
    "\n",
    "    for ep in tqdm(range(n_episodes), desc='Training'):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        cum_rew = 0\n",
    "        k = 0\n",
    "        while not done and k < max_steps_per_episode:\n",
    "            action, value, logp_a = agent.decide(state) \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # track experience\n",
    "            agent.buf.push(state, action, reward, value, logp_a)\n",
    "\n",
    "            cum_rew += reward\n",
    "            state = next_state\n",
    "            k += 1\n",
    "\n",
    "        # if episode finished with done signal, the long term value should be 0\n",
    "        #  for this terminal state\n",
    "        if done:\n",
    "            last_value = None\n",
    "        else:\n",
    "            _, last_value, _ = self.decide(state)\n",
    "        agent.deliberate(last_value)\n",
    "        #train_logs['rewards'].append(cum_rew)\n",
    "    #rewards = np.array(train_logs['rewards']).ravel()\n",
    "    train_logs['envs'] += [env]\n",
    "    train_logs['models'] += [agent]\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ba6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_trends(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70642580",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610401c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc04c0d",
   "metadata": {},
   "source": [
    "### Demo: StableBaselines3 usage\n",
    "\n",
    "Alternatively, in the real world, you would use readily available Python packages such as [stable-baselines3](https://github.com/DLR-RM/stable-baselines3) for employment of state-of-the-art algorithms.\n",
    "In what follows below, the PPO algorithm as utilized by stable-baselines3 is showcased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d06d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Pbar(BaseCallback):\n",
    "    \"\"\"This is a callback that helps monitoring the training progress\"\"\"\n",
    "    def __init__(self, total, n_rollout_steps):\n",
    "        super().__init__()\n",
    "        self.n_steps = n_rollout_steps\n",
    "        self.pbar = tqdm(desc='Training', total=total)\n",
    "        \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"This event is triggered before updating the policy.\"\"\"\n",
    "        self.pbar.update(self.n_steps)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    \n",
    "policy_kwargs = dict(activation_fn=torch.nn.Sigmoid, net_arch=[dict(pi=[8], vf=[8])])\n",
    "\n",
    "n_steps = 256  # an episode takes around ~250 steps\n",
    "total_timesteps = n_steps*250  # steps_per_episode * episodes\n",
    "\n",
    "repeats = 5  # repeat experiment x times to assess scatter through random init\n",
    "logs = {'envs': [], 'models': []}\n",
    "for _ in range(repeats):\n",
    "    \n",
    "    env = Monitor(GoddardEnv())\n",
    "    model = PPO('MlpPolicy', env, n_steps=n_steps, n_epochs=10,  verbose=0, device='cpu', learning_rate=1e-4, batch_size=n_steps, policy_kwargs=policy_kwargs,\n",
    "               clip_range=0.1, clip_range_vf=0.1, gamma=0.999)\n",
    "    model = model.learn(total_timesteps=total_timesteps, callback=Pbar(total_timesteps, n_steps))\n",
    "    logs['envs'] += [env]\n",
    "    logs['models'] += [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6964c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_reward_trends(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694b064",
   "metadata": {},
   "source": [
    "Obviously, the chosen architecture and hyper parameters do well often, but can also fail abruptly. \n",
    "What might be the reason for these failures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31113c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1\n",
    "max_steps_per_episode = 500\n",
    "tst_logs = {'rewards': []}\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cum_rew = 0\n",
    "    k = 0\n",
    "    while not done and k < max_steps_per_episode:\n",
    "        action, _ = model.predict(state, deterministic=True) #\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        cum_rew += reward\n",
    "        k += 1\n",
    "\n",
    "    tst_logs['rewards'].append(cum_rew)\n",
    "rewards = np.array(tst_logs['rewards']).ravel()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean reward ± std.dev: {rewards.mean():.6f} ± {rewards.std()}, best: {rewards.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.scatter(np.random.randn(rewards.size), rewards, label='Cum. Reward')\n",
    "# source: https://github.com/osannolik/gym-goddard/blob/master/gym_goddard/envs/optimal_control.ipynb\n",
    "plt.axhline(0.0122079818367078, ls='--', color='red', label='optimal')\n",
    "# own experiments\n",
    "plt.axhline(0.00995, ls=':', color='blue', label='avg. with random actions')\n",
    "plt.axhline(0.01148, ls='-.', color='blue', label='best with random actions')\n",
    "plt.xlabel('Runs')\n",
    "plt.ylabel('Reached height')\n",
    "plt.ylim(0, None)\n",
    "plt.xticks([])\n",
    "plt.title('Comparison between PPO performance and some baselines')\n",
    "_ = plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
